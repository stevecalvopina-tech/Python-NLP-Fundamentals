{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Bag of Words\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Learn how to convert text data into numbers through a Bag-of-Words approach.\n",
    "* Understand the TF-IDF algorithm and how it complements the Bag-of-Words representation.\n",
    "* Implement Bag-of-Words and TF-IDF using the `sklearn` package and understand its parameter settings.\n",
    "* Use the numerical representations of text data to perform sentiment analysis.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br> \n",
    "\n",
    "### Sections\n",
    "1. [Exploratory Data Analysis](#section1)\n",
    "2. [Preprocessing](#section2)\n",
    "3. [The Bag-of-Words Representation](#section3)\n",
    "4. [Term Frequency-Inverse Document Frequency](#section4)\n",
    "5. [Sentiment Classification Using the TF-IDF Representation](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e8a36-bd58-4c24-8593-03a0ea70deed",
   "metadata": {},
   "source": [
    "In the previous part, we learned how to perform text preprocessing. However, we didn't move beyond the text data itself. If we're interested in doing any computational analysis on the text data, we still need approaches to convert the text into a **numeric representation**.\n",
    "\n",
    "In Part 2 of our workshop series, we'll explore one of the most straightforward ways to generate a numeric representation from text: the **bag-of-words** (BoW). We will implement the BoW representation to transform our airline tweets data, and then build a classifier to explore what it tells us about the sentiment of the tweets. At the heart of the bag-of-words approach lies the assumption that the frequency of specific tokens is informative about the semantics and sentiment underlying the text.\n",
    "\n",
    "We'll make heavy use of the `scikit-learn` package to do so, as it provides a nice framework for constructing the numeric representation.\n",
    "\n",
    "Let's install `scikit-learn` first«É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a3a0d-66f4-44e5-8dd6-5f441146014d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.8/8.7 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.4/8.7 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.7/8.7 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 8.0 MB/s  0:00:01\n",
      "Downloading scipy-1.16.1-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/38.5 MB 13.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 2.4/38.5 MB 13.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.6/38.5 MB 11.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 8.9/38.5 MB 11.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.2/38.5 MB 10.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 10.7/38.5 MB 9.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.0/38.5 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 18.4/38.5 MB 11.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.7/38.5 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.3/38.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.9/38.5 MB 10.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 26.0/38.5 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.4/38.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.0/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.9/38.5 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.2/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 10.2 MB/s  0:00:03\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to install the package\n",
    "%pip install scikit-learn #instalar la librer√≠a scikit-learn usada para machine learning en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed437f-9767-43b7-abc5-159aa4339a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from click->NLTK) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spaCy in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (0.16.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spaCy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from jinja2->spaCy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to install the NLP packages introduced in Part 1\n",
    "%pip install NLTK  # instala NLTK\n",
    "%pip install spaCy # instala spaCy\n",
    "!python -m spacy download en_core_web_sm # descarga el modelo de lenguaje en ingl√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3862ffd-918f-4184-8c90-8a39a8a2a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other packages\n",
    "import re  # Expresiones regulares (para limpieza de texto)\n",
    "import numpy as np # Operaciones num√©ricas y manejo de arreglos\n",
    "import pandas as pd # Manejo de datos en tablas (DataFrames)\n",
    "import matplotlib.pyplot as plt # Gr√°ficas y visualizaci√≥n b√°sica\n",
    "import seaborn as sns # Visualizaci√≥n avanzada y est√©tica mejorada sobre matplotlib\n",
    "from string import punctuation # Lista de signos de puntuaci√≥n, √∫til para limpieza de texto\n",
    "%matplotlib inline # Mostrar gr√°ficos dentro del Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ea4a5-7c28-4557-acdd-afe8a97b7235",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "Before we ever do any preprocessing or modeling, we always should perform exploratory data analysis to familiarize ourselves with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4190e351-97b7-4c5b-866e-07aa6cbd42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "tweets_path = '../data/airline_tweets.csv' # Ruta donde est√° guardado el archivo CSV\n",
    "\n",
    "tweets = pd.read_csv(tweets_path, sep=',') # Carga el archivo CSV en un DataFrame de pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head() # Mostra las primeras 5 filas del DataFrame tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "As a refresher, each row in this dataframe correponds to a tweet. The following columns are of main interests to us. There are other columns containing metadata of the tweet, such as the author of the tweet, when it was created, the timezone of the user, and others, which we will set aside for now. \n",
    "\n",
    "- `text` (`str`): the text of the tweet.\n",
    "- `airline_sentiment` (`str`): the sentiment of the tweet, labeled as \"neutral,\" \"positive,\" or \"negative.\" \n",
    "- `airline` (`str`): the airline that is tweeted about.\n",
    "- `retweet count` (`int`): how many times the tweet was retweeted.\n",
    "\n",
    "To prepare us for sentiment classification, we'll partition the dataset to focus on the \"positive\" and \"negative\" tweets for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1faaf90-8c01-4d25-9468-90c01823f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra los tweets que no sean 'neutral' en 'airline_sentiment' y reinicia los √≠ndices\n",
    "tweets = tweets[tweets['airline_sentiment'] != 'neutral'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6b039-53e7-4afe-a9e0-b3522c12b2d7",
   "metadata": {},
   "source": [
    "Let's take a look at a few tweets first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "@VirginAmerica and it's a really big bad thing about it\n",
      "@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      "@VirginAmerica yes, nearly every time I fly VX this ‚Äúear worm‚Äù won‚Äôt go away :)\n"
     ]
    }
   ],
   "source": [
    "# Print first five tweets\n",
    "for idx in range(5):\n",
    "    print(tweets['text'].iloc[idx]) # Accede al texto del tweet en la posici√≥n 'idx' y lo imprime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "We can already see that some of these tweets contain negative sentiment‚Äîhow can we tell this is the case? \n",
    "\n",
    "Next, let's take a look at the distribution of sentiment labels in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01955158-6954-447a-acb6-2989d02a49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Make a bar plot showing the count of tweet sentiments\n",
    "sns.countplot(\n",
    "            data=tweets,  # DataFrame de origen\n",
    "              x='airline_sentiment', # Columna que define las categor√≠as del eje x\n",
    "              color='cornflowerblue', # Color de las barras\n",
    "              order=['positive', 'negative']); # Orden de las categor√≠as en el gr√°fico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab45abf-adf4-4f5e-ae09-75f6c4fd50d1",
   "metadata": {},
   "source": [
    "It looks like the majority of the tweets in this dataset are expressing negative sentiment!\n",
    "\n",
    "Let's take a look at what gets more retweeted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428ddde7-af73-4eb6-92c9-041a1791ca59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    0.093375\n",
       "neutral     0.060987\n",
       "positive    0.069403\n",
       "Name: retweet_count, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "tweets_path = '../data/airline_tweets.csv'\n",
    "\n",
    "# Cargar el CSV en un DataFrame llamado 'tweets'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')\n",
    "\n",
    "# Verificar que se carg√≥ correctamente\n",
    "tweets.head()\n",
    "\n",
    "# Get the mean retweet count for each sentiment\n",
    "# Agrupa los tweets por la columna 'airline_sentiment'. Luego selecciona la columna 'retweet_count' y calcula la media (promedio) de cada grupo\n",
    "tweets.groupby('airline_sentiment')['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31f3bc-257c-48a8-86a0-fd0d7c3e8cb3",
   "metadata": {},
   "source": [
    "Negative tweets are clearly retweeted more often than positive ones!\n",
    "\n",
    "Let's see which airline receives most negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12aa9f2d-d655-494a-bb72-08ad973518f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US Airways</th>\n",
       "      <td>0.776862</td>\n",
       "      <td>0.130793</td>\n",
       "      <td>0.092345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>0.710402</td>\n",
       "      <td>0.167814</td>\n",
       "      <td>0.121783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United</th>\n",
       "      <td>0.688906</td>\n",
       "      <td>0.182365</td>\n",
       "      <td>0.128728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest</th>\n",
       "      <td>0.490083</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>0.235537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>0.429793</td>\n",
       "      <td>0.325383</td>\n",
       "      <td>0.244824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgin America</th>\n",
       "      <td>0.359127</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.301587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "airline_sentiment  negative   neutral  positive\n",
       "airline                                        \n",
       "US Airways         0.776862  0.130793  0.092345\n",
       "American           0.710402  0.167814  0.121783\n",
       "United             0.688906  0.182365  0.128728\n",
       "Southwest          0.490083  0.274380  0.235537\n",
       "Delta              0.429793  0.325383  0.244824\n",
       "Virgin America     0.359127  0.339286  0.301587"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the proportion of negative tweets by airline\n",
    "# 1. Agrupa por 'airline' y 'airline_sentiment' y cuenta el n√∫mero de tweets por grupo\n",
    "# 2. Divide por el total de tweets de cada aerol√≠nea\n",
    "proportions = tweets.groupby(['airline', 'airline_sentiment']).size() / tweets.groupby('airline').size()\n",
    "proportions.unstack().sort_values('negative', ascending=False) # Reorganiza los datos en formato tabla (sentimientos como columnas) y ordena por la proporci√≥n de negativos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042419e-9c41-40e7-8dbf-47bd1e2ad45a",
   "metadata": {},
   "source": [
    "It looks like people are most dissatified with US Airways, followed by American Airline, both having over 85\\% negative tweets!\n",
    "\n",
    "A lot of interesting discoveries could be made if you want to explore more about the data. Now let's return to our task of sentiment analysis. Before that, we need to preprocess the text data so that they are in a standard format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Preprocessing\n",
    "\n",
    "We spent much of Part 1 learning how to preprocess data. Let's apply what we learned! Looking at some of the tweets above, we can see that while they are in pretty good shape, we can do some additional processing on them.\n",
    "\n",
    "In our pipeline, we'll omit the tokenization process since we will perform it in a later step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Apply a Text Cleaning Pipeline\n",
    "\n",
    "Write a function called `preprocess` that performs the following steps on a text input:\n",
    "\n",
    "* Step 1: Lowercase the text input.\n",
    "* Step 2: Replace the following patterns with placeholders:\n",
    "    * URLs &rarr; ` URL `\n",
    "    * Digits &rarr; ` DIGIT `\n",
    "    * Hashtags &rarr; ` HASHTAG `\n",
    "    * Tweet handles &rarr; ` USER `\n",
    "* Step 3: Remove extra blankspace.\n",
    "\n",
    "Here are some hints to guide you through this challenge:\n",
    "\n",
    "* For Step 1, recall from Part 1 that a string method called [`.lower()`](https://docs.python.org/3.11/library/stdtypes.html#str.lower) can be usd to convert text to lowercase. \n",
    "* We have integrated Step 2 into a function called `placeholder`. Run the cell below to import it into your notebook, and you can use it just like any other functions.\n",
    "* For Step 3, we have provided the regex pattern for identifying whitespace characters as well as the correct replacement for extract whitespace. \n",
    "\n",
    "Run your `preprocess` function on `example_tweet` (three cells below) to check if it works. If it does, apply it to the entire `text` column in the tweets dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21738b02-9ab9-4a61-b41f-ff75888aa747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geoco\\OneDrive\\Escritorio\\UIDE\\Practicas\\Analisis_datos\\Python-NLP-Fundamentals\\lessons\\utils.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  digit_pattern = '\\d+'\n",
      "c:\\Users\\geoco\\OneDrive\\Escritorio\\UIDE\\Practicas\\Analisis_datos\\Python-NLP-Fundamentals\\lessons\\utils.py:14: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  digit_pattern = '\\d+'\n"
     ]
    }
   ],
   "source": [
    "from utils import placeholder #importa una funci√≥n o variable llamada placeholder desde un m√≥dulo llamado utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03569f0d-34ba-492d-aa1d-1dce9d34f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "blankspace_pattern = r'\\s+'# Patr√≥n de expresi√≥n regular: uno o m√°s espacios en blanco consecutivos\n",
    "blankspace_repl = ' ' # Reemplazo: un solo espacio\n",
    "\n",
    "def preprocess(text):\n",
    "    '''Create a preprocess pipeline that cleans the tweet data.'''\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()    # Paso 1: Convertir todo el texto a min√∫sculas\n",
    "\n",
    "\n",
    "    # Step 2: Replace patterns with placeholders\n",
    "    text = re.sub(pattern=blankspace_pattern, repl=blankspace_repl, string=text)   # Usamos la funci√≥n re.sub para reemplazar todos los espacios m√∫ltiples por un solo espacio\n",
    "\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = text.strip()    # Paso 3: Eliminar espacios en blanco al inicio y final del texto\n",
    "\n",
    "\n",
    "    return text     # Devolver el texto limpio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8990cefd-5d04-46ba-ada2-29978c28cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\n",
      "==================================================\n",
      "lol @justinbeiber and @billgates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\n"
     ]
    }
   ],
   "source": [
    "import re  # Librer√≠a para expresiones regulares\n",
    "\n",
    "# Define un tweet de ejemplo con menciones, hashtags, URL y n√∫meros\n",
    "example_tweet = 'lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo'\n",
    "\n",
    "# Print the example tweet\n",
    "print(example_tweet) # Imprime el tweet original\n",
    "print(f\"{'='*50}\") # Imprime una l√≠nea separadora\n",
    "\n",
    "# Print the preprocessed tweet\n",
    "print(preprocess(example_tweet))# Esto convierte a min√∫sculas, reemplaza espacios m√∫ltiples por uno solo y elimina espacios al inicio y final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f7bb6a-f064-48cc-b650-12c4ef2fbb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  @virginamerica what @dhepburn said.\n",
       "1    @virginamerica plus you've added commercials t...\n",
       "2    @virginamerica i didn't today... must mean i n...\n",
       "3    @virginamerica it's really aggressive to blast...\n",
       "4    @virginamerica and it's a really big bad thing...\n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the text column and assign the preprocessed tweets to a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))# Guarda los tweets limpios en una nueva columna llamada 'text_processed'\n",
    "\n",
    "tweets['text_processed'].head()# Muestra los primeros cinco tweets preprocesados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acc6-b305-492a-8fde-65b343cb779c",
   "metadata": {},
   "source": [
    "Congratualtions! Preprocessing is done. Let's dive into the bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# The Bag-of-Words Representation\n",
    "\n",
    "The idea of bag-of-words (BoW), as the name suggests, is quite intuitive: we take a document and toss it in a bag. The action of \"throwing\" the document in a bag disregards the relative position between words, so what is \"in the bag\" is essentially \"an unsorted set of words\" [(Jurafsky & Martin, 2024)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf). In return, we have a list of unique words and the frequency of each of them. \n",
    "\n",
    "For example, as shown in the following illustration, the word \"coffee\" appears twice. \n",
    "\n",
    "<img src='../images/bow-illustration-1.png' alt=\"BoW-Part2\" width=\"600\">\n",
    "\n",
    "With a bag-of-words representation, we make heavy use of word frequency but not too much of word order. \n",
    "\n",
    "In the context of sentiment analysis, the sentiment of a tweet is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy,\" it likely conveys positive sentiment, but not always (e.g., \"not happy\" denotes the opposite sentiment). When these words come up more often, they'll probably more strongly convey the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "Now let's implement the idea of bag-of-words. Before we dive deeper, let's step back for a moment. In practice, text analysis often involves handling many documents; from now on, we use the term **document** to represent a piece of text on which we perform analysis. It could be a phrase, a sentence, a tweet, or any other text‚Äîas long as it can be represented by a string, the length dosen't really matter. \n",
    "\n",
    "Imagine we have four documents (i.e., the four phrases shown above), and we toss them all in the bag. Instead of a word-frequency list, we'd expect a document-term matrix (DTM) in return. In a DTM, the word list is the **vocabulary** (V) that holds all unique words occur across the documents. For each **document** (D), we count the number of occurence of each word in the vocabulary, and then plug the number into the matrix. In other words, the DTM we will construct is a $D \\times V$ matrix, where each row corresponds to a document, and each column corresponds to a token (or \"term\").\n",
    "\n",
    "The unique tokens in this set of documents, arranged in alphabetical order, form the columns. For each document, we mark the occurence of each word present in the document. The numerical representation for each document is a row in the matrix. For example, the first document, \"the coffee roaster,\" has the numerical representation $[0, 1, 0, 0, 0, 1, 1, 0]$.\n",
    "\n",
    "Note that the left index column now displays these documents as text, but typically we would just assign an index to each of them. \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{americano} & \\text{coffee} & \\text{iced} & \\text{light} & \\text{roast} & \\text{roaster} & \\text{the} & \\text{time} \\\\\\hline\n",
    "\\text{the coffee roaster} &0 &1\t&0\t&0\t&0\t&1\t&1\t&0 \\\\ \n",
    "\\text{light roast} &0 &0\t&0\t&1\t&1\t&0\t&0\t&0 \\\\\n",
    "\\text{iced americano} &1 &0\t&1\t&0\t&0\t&0\t&0\t&0 \\\\\n",
    "\\text{coffee time} &0 &1\t&0\t&0\t&0\t&0\t&0\t&1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To create a DTM, we will use `CountVectorizer` from the package `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer# Importa CountVectorizer de scikit-learn, que convierte texto en una matriz de conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989781d-6b40-417a-be70-eeba05cd8a50",
   "metadata": {},
   "source": [
    "The following illustration depicts the three-step workflow of creating a DTM with `CountVectorizr`.\n",
    "\n",
    "<img src='../images/CountVectorizer1.png' alt=\"CountVectorizer\" width=\"500\">\n",
    "\n",
    "Let's walk through these steps with the toy example shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34174034-46b9-43e2-a511-5972d378cb00",
   "metadata": {},
   "source": [
    "### A Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4da2bd3d-0460-4b5f-9b9e-02940db0d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example containing four documents\n",
    "test = ['the coffee roaster',# Documento 1: contiene las palabras 'the', 'coffee', 'roaster'\n",
    "        'light roast',  # Documento 2: contiene las palabras 'light', 'roast'\n",
    "        'iced americano', # Documento 3: contiene las palabras 'iced', 'americano'\n",
    "        'coffee time']  # Documento 4: contiene las palabras 'coffee', 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c1d3-fcee-4e20-b9a7-17306ebd5fc2",
   "metadata": {},
   "source": [
    "The first step is to initialize a `CountVectorizer` object. Within the round paratheses, we can specify parameter settings if desired. Let's take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and see what options are available.  \n",
    "\n",
    "For now we can just leave it blank to use the default settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de3fe6a-9abf-4e11-aad1-e54c891567bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer() # Este objeto se utilizar√° para transformar la lista de documentos de texto en una matriz de conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a7d0d-0bfc-4fb9-8e5f-e91e39797fb5",
   "metadata": {},
   "source": [
    "The second step is to `fit` this `CountVectorizer` object to the data, which means creating a vocabulary of tokens from the set of documents. Thirdly, we `transform` our data according to the \"fitted\" `CountVectorizer` object, which means taking each of the document and counting the occurrences of tokens according to the vocabulary established during the \"fitting\" step.\n",
    "\n",
    "It may sound a bit complex but steps 2 and 3 can be done in one swoop using a `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da1bbad4-bb1a-4b92-9096-6e17558b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform to create a DTM\n",
    "test_count = vectorizer.fit_transform(test) # Ajusta (fit) el CountVectorizer al corpus y transforma (transform) los documentos en una matriz de conteo de palabras (DTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d3b65-4e98-48bf-87d2-399457f4939c",
   "metadata": {},
   "source": [
    "The return of `fit_transform` is supposed to be the DTM. \n",
    "\n",
    "Let's take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb044001-8eb2-4489-b025-2d8e2d4bfee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 9 stored elements and shape (4, 8)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_count #Es la matriz dispersa que resulta de aplicar CountVectorizer a la lista test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9817b09-a806-42c4-9436-822cc27a38b9",
   "metadata": {},
   "source": [
    "Apparently we've got a \"sparse matrix\"‚Äîa matrix that contains a lot of zeros. This makes sense. For each document, there are words that don't occur at all, and these are counted as zero in the DTM. This sparse matrix is stored in a \"Compressed Sparse Row\" format, a memory-saving format designed for handling sparse matrices. \n",
    "\n",
    "Let's convert it to a dense matrix, where those zeros are probably represented, as in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03a238-87d8-40c9-b20e-66e7c9b6576b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DTM to a dense matrix \n",
    "test_count_dense = test_count.todense() # Convierte la matriz dispersa (DTM) en una matriz densa para poder visualizar los conteos de palabras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b58a63-d7f6-4b9f-aadf-4d4fc7341336",
   "metadata": {},
   "source": [
    "So this is our DTM! The matrix is the same as shown above. To make it more reader-friendly, let's convert it to a dataframe. The column names should be tokens in the vocabulary, which we can access with the `get_feature_names_out` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "714de5d3-e37d-4a19-9ade-3c6629e38d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['americano', 'coffee', 'iced', 'light', 'roast', 'roaster', 'the',\n",
       "       'time'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the vocabulary\n",
    "vectorizer.get_feature_names_out()# Devuelve un array con todas las palabras √∫nicas de los documentos en orden alfab√©tico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a7729a2-ca2e-4de7-8795-74dfedb7a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DTM dataframe\n",
    "# Cada fila representa un documento\n",
    "# Cada columna representa una palabra del vocabulario\n",
    "# Los valores indican cu√°ntas veces aparece cada palabra en cada documento\n",
    "test_dtm = pd.DataFrame(data=test_count.todense(), # Convertimos la matriz dispersa a una matriz densa\n",
    "                        columns=vectorizer.get_feature_names_out()) # Nombramos las columnas con las palabras del vocabulario\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781da407-f394-40f2-9d45-1fac39f02047",
   "metadata": {},
   "source": [
    "Here it is! The DTM of our toy data is now a dataframe. The index of `test_dtm` corresponds to the position of each document in the `test` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e41dd243-cd2e-43c3-80f8-5eaab6e64210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>americano</th>\n",
       "      <th>coffee</th>\n",
       "      <th>iced</th>\n",
       "      <th>light</th>\n",
       "      <th>roast</th>\n",
       "      <th>roaster</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   americano  coffee  iced  light  roast  roaster  the  time\n",
       "0          0       1     0      0      0        1    1     0\n",
       "1          0       0     0      1      1        0    0     0\n",
       "2          1       0     1      0      0        0    0     0\n",
       "3          0       1     0      0      0        0    0     1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm # DataFrame que representa la Document-Term Matrix (DTM) de los documentos de ejemplo 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a03b4-94fa-4fe7-8f5d-7280e31b9bc4",
   "metadata": {},
   "source": [
    "Hopefully this toy example provides a clear walkthrough of creating a DTM.\n",
    "\n",
    "Now it's time for our tweets data!\n",
    "\n",
    "### DTM for Tweets\n",
    "\n",
    "We'll begin by initializing a `CountVectorizer` object. In the following cell, we have included a few parameters that people often adjust. These parameters are currently set to their default values.\n",
    "\n",
    "When we construct a DTM, the default is to lowercase the input text. If nothing is provided for `stop_words`, the default is to keep them. The next three parameters are used to control the size of the vocabulary, which we'll return to in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True,# Convierte todo el texto a min√∫sculas antes de tokenizar\n",
    "                             stop_words=None, # No elimina ninguna palabra por defecto (no usa stopwords)\n",
    "                             min_df=1, # La palabra debe aparecer al menos en 1 documento para incluirse en el vocabulario\n",
    "                             max_df=1.0, # La palabra puede aparecer como m√°ximo en el 100% de los documentos\n",
    "                             max_features=None)  # No hay l√≠mite en el n√∫mero de caracter√≠sticas (palabras) a extraer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f85e76ea-bc54-4775-bcda-432a03d2c96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 234281 stored elements and shape (14640, 15051)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform to create DTM\n",
    "# Ajusta (fit) el CountVectorizer al corpus de tweets preprocesados y transforma (transform) cada tweet en un vector de conteo de palabras (DTM)\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "counts # counts ahora contiene la Document-Term Matrix (DTM) en formato de matriz dispersa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run if you have limited memory - this includes DataHub and Binder\n",
    "np.array(counts.todense()) # Convierte la matriz dispersa 'counts' a una matriz densa de NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens\n",
    "tokens = vectorizer.get_feature_names_out() # Estos tokens corresponden a las columnas de la Document-Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43620587-3795-4434-8f1f-145c81b93706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DTM\n",
    "# Cada fila corresponde a un tweet (usando el mismo √≠ndice que el DataFrame original)\n",
    "# Cada columna corresponde a una palabra del vocabulario\n",
    "first_dtm = pd.DataFrame(data=counts.todense(), # Convertimos la matriz dispersa a matriz densa\n",
    "                         index=tweets.index, # Usamos los √≠ndices originales de los tweets\n",
    "                         columns=tokens)  # Nombres de columnas = palabras √∫nicas aprendidas por CountVectorizer\n",
    "\n",
    "# Print the shape of DTM\n",
    "print(first_dtm.shape) # Muestra (cantidad de tweets, cantidad de palabras √∫nicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd257d5-4244-436c-afe7-5688232caf8f",
   "metadata": {},
   "source": [
    "If we leave the `CountVectorizer` to the default setting, the vocabulary size of the tweet data is 8751. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3604ec-d909-4238-9a3f-67e7d4ae2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dtm.head() # Muestra las primeras 5 filas del DataFrame `first_dtm`\n",
    "                   # Cada fila = un tweet\n",
    "                   # Cada columna = una palabra del vocabulario\n",
    "                   # Cada valor = n√∫mero de veces que aparece esa palabra en ese tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d34e2-52f8-4419-b4c7-ed20dbd5df89",
   "metadata": {},
   "source": [
    "Most of the tokens have zero occurences at least in the first five tweets. \n",
    "\n",
    "Let's take a closer look at the DTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent tokens\n",
    "# 1. first_dtm.sum() ‚Üí suma los valores de cada columna, es decir, cuenta cu√°ntas veces aparece cada palabra en todos los tweets\n",
    "# 2. .sort_values(ascending=False) ‚Üí ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) ‚Üí muestra las 10 palabras m√°s frecuentes\n",
    "first_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7f1c9-dd66-49f2-b337-01253da551d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_exact_                     1\n",
       "mightmismybrosgraduation    1\n",
       "midterm                     1\n",
       "midnite                     1\n",
       "midland                     1\n",
       "michelle                    1\n",
       "michele                     1\n",
       "michael                     1\n",
       "mhtt                        1\n",
       "mgmt                        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least frequent tokens\n",
    "# 1. first_dtm.sum() ‚Üí suma los valores de cada columna, es decir, cuenta cu√°ntas veces aparece cada palabra en todos los tweets\n",
    "# 2. .sort_values(ascending=True) ‚Üí ordena las palabras de menor a mayor frecuencia\n",
    "# 3. .head(10) ‚Üí muestra las 10 palabras menos frecuentes\n",
    "first_dtm.sum().sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d230f79-e752-4e32-93db-4f013287f8e2",
   "metadata": {},
   "source": [
    "It is not surprising to see \"user\" and \"digit\" to be among the most frequent tokens as we replaced each idiosyncratic one with these placeholders. The rest of the most frequent tokens are mostly stop words.\n",
    "\n",
    "Perhaps a more interesting pattern is to look for which token appears most in any given tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8f4d8-4c88-4155-a6c5-c72a5b4e8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame() # Crear un DataFrame vac√≠o para almacenar los tokens m√°s frecuentes por tweet\n",
    "\n",
    "# Retrieve the index of the tweet where a token appears most frequently\n",
    "counts['token'] = first_dtm.idxmax(axis=1)# idxmax(axis=1) devuelve el nombre de la columna con el valor m√°ximo por fila\n",
    "\n",
    "# Retrieve the number of occurrence\n",
    "# Recuperar el nombre del token que aparece con mayor frecuencia en cada tweet \n",
    "counts['number'] = first_dtm.max(axis=1) # max(axis=1) devuelve el valor m√°ximo por fila (frecuencia del token)\n",
    "\n",
    "# Filter out placeholders\n",
    "# Filtrar los tokens que son placeholders (digit, hashtag, user)\n",
    "# y mostrar los 10 tokens m√°s frecuentes despu√©s del filtrado\n",
    "counts[(counts['token']!='digit')\n",
    "       & (counts['token']!='hashtag')\n",
    "       & (counts['token']!='user')].sort_values('number', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdac4ef-6b9d-4aad-9b24-c70f6c2eb8f0",
   "metadata": {},
   "source": [
    "It looks like among all tweets, at most a token appears six times, and it is either the word \"It\" or the word \"worst.\" \n",
    "\n",
    "Let's go back to our tweets dataframe and locate the 918th tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e7cacd8-1fb3-4f0d-a744-4ee0994a089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@united Agent in LAS letting 20 customers know they can't help them rebook delayed flight to DEN #unfriendlyskies http://t.co/QuzVmK2rTR\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve 918th tweet: \"worst\"\n",
    "# Recuperar el tweet en la posici√≥n 918 del DataFrame\n",
    "# Muestra el contenido del tweet original en la columna 'text'\n",
    "tweets.iloc[918]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8e37-4880-4565-b6fc-7e7c96958f0f",
   "metadata": {},
   "source": [
    "## Customize the `CountVectorizer`\n",
    "\n",
    "So far we've always used the default parameter setting to create our DTMs, but in many cases we may want to customize the `CountVectorizer` object. The purpose of doing so is to further filter out unnecessary tokens. In the example below, we tweak the following parameters:\n",
    "\n",
    "- `stop_words = 'english'`: ignore English stop words \n",
    "- `min_df = 2`: ignore words that don't occur at least twice\n",
    "- `max_df = 0.95`: ignore words if they appear in more than 95\\% of the documents\n",
    "\n",
    "üîî **Question**: Let's pause for a minute to discuss whether it sounds reasonable to set these parameters! What do you think?\n",
    "\n",
    "Oftentimes, we are not interested in words whose frequencies are either too low or too high, so we use `min_df` and `max_df` to filter them out. Alternatively, we can define our vocabulary size as $N$ by setting `max_features`. In other words, we tell `CountVectorizer` to only consider the top $N$ most frequent tokens when constructing the DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37a0a93e-9dd8-43dc-a82c-06a24bf02bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the parameter setting\n",
    "# Crear un objeto CountVectorizer con par√°metros personalizados\n",
    "vectorizer = CountVectorizer(lowercase=True, # Convierte todo el texto a min√∫sculas antes de tokenizar\n",
    "                             stop_words='english', # Elimina las palabras vac√≠as (stop words)\n",
    "                             min_df=2,  # Ignora palabras que aparecen en menos de 2 documentos\n",
    "                             max_df=0.95, # Ignora palabras que aparecen en m√°s del 95% de los documentos\n",
    "                             max_features=None) # No limita el n√∫mero m√°ximo de palabras (todas se incluyen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b53e5ecf-7be3-4915-9d11-fd3edb913400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit, transform, and get tokens\n",
    "# Ajustar el vectorizador al texto preprocesado y transformar los tweets en una matriz de conteo\n",
    "counts = vectorizer.fit_transform(tweets['text_processed']) # Devuelve una matriz dispersa donde cada fila = tweet, cada columna = palabra, y cada celda = n√∫mero de apariciones\n",
    "tokens = vectorizer.get_feature_names_out() # Obtener los tokens (palabras √∫nicas) identificadas por el CountVectorizer\n",
    "\n",
    "# Create the second DTM\n",
    "# Crear un DataFrame de la segunda Document-Term Matrix (DTM)\n",
    "second_dtm = pd.DataFrame(data=counts.todense(), # Convertir la matriz dispersa a matriz densa\n",
    "                          index=tweets.index,  # Mantener los √≠ndices originales de los tweets\n",
    "                          columns=tokens) # Usar los tokens como nombres de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e66bc-2eaa-4642-8848-74459948084b",
   "metadata": {},
   "source": [
    "Our second DTM has a substantially smaller vocabulary compared to the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb598-fa81-4111-9e36-7172d8034713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la forma (n√∫mero de filas y columnas) de la primera DTM\n",
    "# Cada fila = un tweet, cada columna = una palabra\n",
    "print(first_dtm.shape)\n",
    "# Imprimir la forma (n√∫mero de filas y columnas) de la segunda DTM\n",
    "# Esta DTM usa par√°metros de CountVectorizer m√°s estrictos (stop words, min_df, max_df)\n",
    "print(second_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8deabb2-20eb-4047-b592-48cb1564fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>0510</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>...</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zik2uoxgnw</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 6144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00am  00pm  02  03  05  0510  05am  05pm  ...  yvonne  yvr  yyj  \\\n",
       "0   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "1   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "2   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "3   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "4   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "\n",
       "   yyz  zero  zik2uoxgnw  zkatcher  zone  zoom  zurich  \n",
       "0    0     0           0         0     0     0       0  \n",
       "1    0     0           0         0     0     0       0  \n",
       "2    0     0           0         0     0     0       0  \n",
       "3    0     0           0         0     0     0       0  \n",
       "4    0     0           0         0     0     0       0  \n",
       "\n",
       "[5 rows x 6144 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra las primeras 5 filas del DataFrame `second_dtm`\n",
    "# Cada fila = un tweet\n",
    "# Cada columna = una palabra del vocabulario filtrado por CountVectorizer\n",
    "# Cada valor = n√∫mero de veces que aparece esa palabra en ese tweet\n",
    "second_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fe2c3-ec90-4027-8c7f-417327a33a27",
   "metadata": {},
   "source": [
    "The most frequent token list now includes words that make more sense to us, such as \"cancelled\" and \"service.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffa7bf4e-640b-49bc-b64b-721140f67f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "united          4164\n",
       "flight          3939\n",
       "usairways       3053\n",
       "americanair     2964\n",
       "southwestair    2461\n",
       "jetblue         2395\n",
       "http            1155\n",
       "thanks          1083\n",
       "cancelled       1065\n",
       "just             974\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las 10 palabras m√°s frecuentes en la segunda DTM\n",
    "# 1. second_dtm.sum() ‚Üí suma los valores de cada columna, contando cu√°ntas veces aparece cada palabra en todos los tweets\n",
    "# 2. .sort_values(ascending=False) ‚Üí ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) ‚Üí muestra las 10 palabras m√°s frecuentes\n",
    "second_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5145-d505-4e36-9a39-a40d25d8ec6f",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Lemmatize the Text Input\n",
    "\n",
    "Recall from Part 1 that we introduced using `spaCy` to perform lemmatization, i.e., to \"recover\" the base form of a word. This process will reduce vocabulary size by keeping word variations minimal‚Äîa smaller vocabularly may help improve model performance in sentiment classification. \n",
    "\n",
    "Now let's implement lemmatization on our tweet data and use the lemmatized text to create a third DTM. \n",
    "\n",
    "Complete the function `lemmatize_text`. It requires a text input and returns the lemmas of all tokens. \n",
    "\n",
    "Here are some hints to guide you through this challenge:\n",
    "\n",
    "- Step 1: initialize a list to hold lemmas\n",
    "- Step 2: apply the `nlp` pipeline to the input text\n",
    "- Step 3: iterate over tokens in the processed text and retrieve the lemma of the token\n",
    "    - HINT: lemmatization is one of the linguistic annotations that the `nlp` pipeline automatically does for us. We can use `token.lemma_` to access the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da610560-62c3-48ab-a1b2-25e0b589bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy # Importa la librer√≠a spaCy para procesamiento de lenguaje natural\n",
    "nlp = spacy.load('en_core_web_sm') # Carga el modelo en ingl√©s peque√±o 'en_core_web_sm' para tokenizaci√≥n, lematizaci√≥n y reconocimiento de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98ead266-30f3-48ad-bc51-c1685487f000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    '''Lemmatize the text input with spaCy annotations.'''\n",
    "\n",
    "    # Step 1: Initialize an empty list to hold lemmas\n",
    "    lemma = [] \n",
    "\n",
    "    # Step 2: Apply the nlp pipeline to input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Step 3: Iterate over tokens in the text to get the token lemma\n",
    "    for token in doc:\n",
    "        lemma.append(token.lemma_)# token.lemma_ devuelve la forma base del token\n",
    "\n",
    "    # Step 4: Join lemmas together into a single string\n",
    "    text_lemma = ' '.join(lemma)\n",
    "    \n",
    "    return text_lemma # Devolver el texto lematizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36aab6-35dd-42a2-9b38-b7c432f021c6",
   "metadata": {},
   "source": [
    "Let's apply the function to the following example tweet first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "742e82bb-5c42-4fa8-9101-5a0ea908db25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@virginamerica awaiting my return phone call, just would prefer to use your online self-service option :(\n",
      "==================================================\n",
      "@virginamerica await my return phone call , just would prefer to use your online self - service option :(\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to an example tweet\n",
    "print(tweets.iloc[33][\"text_processed\"])# Mostra el tweet preprocesado en la posici√≥n 33\n",
    "print(f\"{'='*50}\") # Imprime una l√≠nea separadora\n",
    "# Mostrar el mismo tweet despu√©s de aplicar la funci√≥n de lematizaci√≥n\n",
    "# Cada palabra se convierte a su forma base (por ejemplo: \"running\" ‚Üí \"run\")\n",
    "print(lemmatize_text(tweets.iloc[33]['text_processed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeda987-dc32-4979-b158-c24be7d1a420",
   "metadata": {},
   "source": [
    "And then let's lemmatize the tweet data and save the output to a new column `text_lemmatized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ac128d2-1be5-4ef5-bb50-5b8d44ef8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while!\n",
    "tweets['text_lemmatized'] = tweets['text_processed'].apply(lambda x: lemmatize_text(x))# Aplica la funci√≥n de lematizaci√≥n a todos los tweets preprocesados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02aad6-4e71-4afc-80cf-31d4f39498b2",
   "metadata": {},
   "source": [
    "Now with the `text_lemmatized` column, let's create a third DTM. The parameter setting is the same as the second DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f49d790-3c9d-4dc1-a5c9-72c306630412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>0510</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>...</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zik2uoxgnw</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 5090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00am  00pm  02  03  05  0510  05am  05pm  ...  yvonne  yvr  yyj  \\\n",
       "0   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "1   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "2   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "3   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "4   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "\n",
       "   yyz  zero  zik2uoxgnw  zkatcher  zone  zoom  zurich  \n",
       "0    0     0           0         0     0     0       0  \n",
       "1    0     0           0         0     0     0       0  \n",
       "2    0     0           0         0     0     0       0  \n",
       "3    0     0           0         0     0     0       0  \n",
       "4    0     0           0         0     0     0       0  \n",
       "\n",
       "[5 rows x 5090 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vectorizer (the same param setting as previous)\n",
    "vectorizer = CountVectorizer(lowercase=True,# Convierte todo el texto a min√∫sculas\n",
    "                             stop_words='english', # Eliminar stop words en ingl√©s\n",
    "                             min_df=2, # Ignora palabras que aparecen en menos de 2 documentos\n",
    "                             max_df=0.95, # Ignora palabras que aparecen en m√°s del 95% de los documentos\n",
    "                             max_features=None) # No limita el n√∫mero de palabras\n",
    "\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_lemmatized']) # Ajusta el vectorizador a los tweets lematizados y transformar en matriz de conteo\n",
    "tokens = vectorizer.get_feature_names_out() # Obtiene los tokens (palabras √∫nicas) identificadas por CountVectorizer\n",
    "\n",
    "# Create the third DTM\n",
    "third_dtm = pd.DataFrame(data=counts.todense(), # Convierte la matriz dispersa a matriz densa\n",
    "                         index=tweets.index, # Mantene los √≠ndices originales de los tweets\n",
    "                         columns=tokens) # Usa los tokens como nombres de columnas\n",
    "third_dtm.head() # Mostrar las primeras 5 filas de la tercera DTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859eb04-dbd2-4fa0-9798-65ed7496c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of three DTMs\n",
    "# Imprime las dimensiones de las tres Document-Term Matrices (DTM)\n",
    "# Cada fila = n√∫mero de tweets\n",
    "# Cada columna = n√∫mero de palabras √∫nicas en el vocabulario correspondiente\n",
    "print(first_dtm.shape)   # Primera DTM: sin filtrar stop words ni aplicar frecuencia m√≠nima/m√°xima\n",
    "print(second_dtm.shape)  # Segunda DTM: con stop words y filtros de frecuencia aplicados\n",
    "print(third_dtm.shape)   # Tercera DTM: despu√©s de lematizaci√≥n y filtros, m√°s refinada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94c8ac-e4f4-4b76-afdb-1d4af54a3eee",
   "metadata": {},
   "source": [
    "Let's print the top 10 most frequent tokens as usual. These tokens are now lemmas and their counts also change after lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5745ca29-97ed-4fe1-81db-7e402c8da674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flight          4754\n",
       "americanair     2964\n",
       "united          2591\n",
       "southwestair    2461\n",
       "jetblue         2395\n",
       "usairway        2366\n",
       "thank           1680\n",
       "unite           1574\n",
       "http            1155\n",
       "hour            1152\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most frequent tokens in the third DTM\n",
    "# 1. third_dtm.sum() ‚Üí suma los valores de cada columna, contando cu√°ntas veces aparece cada palabra en todos los tweets lematizados\n",
    "# 2. .sort_values(ascending=False) ‚Üí ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) ‚Üí muestra las 10 palabras m√°s frecuentes\n",
    "third_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16c63e6a-50c3-448a-9a56-a1d193cd6680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "united          4164\n",
       "flight          3939\n",
       "usairways       3053\n",
       "americanair     2964\n",
       "southwestair    2461\n",
       "jetblue         2395\n",
       "http            1155\n",
       "thanks          1083\n",
       "cancelled       1065\n",
       "just             974\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compared to the most frequent tokens in the second DTM\n",
    "# 1. second_dtm.sum() ‚Üí suma los valores de cada columna, contando cu√°ntas veces aparece cada palabra en todos los tweets preprocesados (sin lematizaci√≥n)\n",
    "# 2. .sort_values(ascending=False) ‚Üí ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) ‚Üí muestra las 10 palabras m√°s frecuentes\n",
    "second_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363398-fdf5-456b-ae3d-cae9d5294140",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency \n",
    "\n",
    "So far, we're relying on word frequency to give us information about a document. This assumes if a word appears more often in a document, it's more informative. However, this may not always be the case. For example, we've already removed stop words because they are not informative, despite the fact that they appear many times in a document. We also know the word \"flight\" is among the most frequent words, but it is not that informative, because it appears in many documents. Since we're looking at airline tweets, we shouldn't be surprised to see the word \"flight\"!\n",
    "\n",
    "To remedy this, we use a weighting scheme called **tf-idf (term frequency-inverse document frequency)**. The big idea behind tf-idf is to weight a word not just by its frequency within a document, but also by its frequency in one document relative to the remaining documents. So, when we construct the DTM, we will be assigning each term a **tf-idf score**. Specifically, term $t$ in document $d$ is assigned a tf-idf score as follows:\n",
    "\n",
    "<img src='../images/tf-idf_finalized.png' alt=\"TF-IDF\" width=\"1200\">\n",
    "\n",
    "In essence, the tf-idf score of a word in a document is the product of two components: **term frequency (tf)** and **inverse document frequency (idf)**. The idf acts as a scaling factor. If a word occurs in all documents, then idf equals 1. No scaling will happen. But idf is typically greater than 1, which is the weight we assign to the word to make the tf-idf score higher, so as to highlight that the word is informative. In practice, we add 1 to both the denominator and numerator (\"add-1 smooth\") to prevent any issues with zero occurrences.\n",
    "\n",
    "We can also create a tf-idf DTM using `sklearn`. We'll use a `TfidfVectorizer` this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5e32d8a-c42d-475f-aab4-21eca8b1aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa TfidfVectorizer de scikit-learn, usado para convertir texto en una matriz TF-IDF\n",
    "# TF-IDF pondera las palabras seg√∫n su frecuencia en un documento y su rareza en todo el corpus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d23916c1-5693-456c-b71d-6d9d78d1e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True, # Convierte todo el texto a min√∫sculas antes de vectorizar\n",
    "                             stop_words='english', # Elimina palabras vac√≠as (stop words) en ingl√©s\n",
    "                             min_df=2, # Ignora palabras que aparecen en menos de 2 documentos\n",
    "                             max_df=0.95, # Ignora palabras que aparecen en m√°s del 95% de los documentos\n",
    "                             max_features=None) # No limitar el n√∫mero de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7af5b342-ab18-4766-9561-e38e50cd1e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 119862 stored elements and shape (14640, 5090)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform \n",
    "tf_dtm = vectorizer.fit_transform(tweets['text_lemmatized'])# Ajusta el vectorizador TF-IDF a los tweets lematizados y transforma el texto en una matriz TF-IDF\n",
    "tf_dtm # Muestra la matriz TF-IDF resultante (matriz dispersa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55e509c8-5402-4be0-9143-0e448fff7066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>0510</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>...</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zik2uoxgnw</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 5090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  00am  00pm   02   03   05  0510  05am  05pm  ...  yvonne  yvr  \\\n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "\n",
       "   yyj  yyz  zero  zik2uoxgnw  zkatcher  zone  zoom  zurich  \n",
       "0  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "1  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "2  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "3  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "4  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "\n",
       "[5 rows x 5090 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tf-idf dataframe\n",
    "# Crear un DataFrame TF-IDF a partir de la matriz dispersa\n",
    "tfidf = pd.DataFrame(tf_dtm.todense(),  # Convertir la matriz TF-IDF dispersa a matriz densa\n",
    "                     columns=vectorizer.get_feature_names_out(), # Usar los tokens (palabras) como nombres de columnas\n",
    "                     index=tweets.index)  # Mantener los √≠ndices originales de los tweets\n",
    "tfidf.head() # Mostrar las primeras 5 filas del DataFrame TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba13ea-c429-4ff1-a9a2-abf27c4d0888",
   "metadata": {},
   "source": [
    "You may have noticed that the vocabulary size is the same as we saw in Challenge 2. This is because we used the same parameter setting when creating the vectorizer. But the values in the matrix are different‚Äîthey are tf-idf scores instead of raw counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58c360-5c55-4fa0-8c55-1f00e68baa9a",
   "metadata": {},
   "source": [
    "## Interpret TF-IDF Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad233d-ebc1-420f-9b67-c227c48f3e60",
   "metadata": {},
   "source": [
    "Let's take a look the document where a term has the highest tf-idf values. We'll use the `.idxmax()` method to find the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "995b511a-d448-4cfb-a6a0-22a465efd8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00            10280\n",
       "000            3288\n",
       "00am          10222\n",
       "00pm          11271\n",
       "02             7905\n",
       "              ...  \n",
       "zik2uoxgnw    11888\n",
       "zkatcher       8389\n",
       "zone           3975\n",
       "zoom           4970\n",
       "zurich         2727\n",
       "Length: 5090, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the document\n",
    "tfidf.idxmax() # Obtener el √≠ndice del tweet donde cada palabra tiene su valor TF-IDF m√°ximo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc0249-7c68-42ee-8290-ff41715e346b",
   "metadata": {},
   "source": [
    "For example, the term \"worst\" occurs most distinctively in the 918th tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09b222fb-ad8c-4767-a974-dd261370a06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(10265)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idxmax()['worst']# Obtener el √≠ndice del tweet donde la palabra 'worst' tiene el valor TF-IDF m√°s alto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a48bc-dc93-481b-ba49-29876fc577fb",
   "metadata": {},
   "source": [
    "Recall that this is the tweet where the word \"worst\" appears six times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "079ee0e0-476f-4236-ba8a-615ba7a0efe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@united agent in las letting 20 customers know they can't help them rebook delayed flight to den #unfriendlyskies http://t.co/quzvmk2rtr\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text_processed'].iloc[918]# Obtiene el tweet preprocesado en la posici√≥n 918\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd06bbc-e2fc-49e4-9354-efdaca5cfbd3",
   "metadata": {},
   "source": [
    "How about \"cancel\"? Let's take a look at another example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f809df1a-1178-4272-a415-42edb20173b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7840)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idxmax()['cancel'] # Obtener el √≠ndice del tweet donde la palabra 'cancel' tiene el valor TF-IDF m√°s alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8093b6a7-54ca-468a-9376-b3c0be0b6f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@southwestair you need to get your act together. you new this morning at 830 our plane was malfunctioning. yet i've been delayed 3 times ..\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text_processed'].iloc[5945]# Obtiene el tweet preprocesado en la posici√≥n 5945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dcecd-dc8c-43a9-952d-5bc84a307b07",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Words with Highest Mean TF-IDF scores\n",
    "\n",
    "We have obtained tf-idf values for each term in each document. But what do these values tell us about the sentiments of tweets? Are there any words that are  particularly informative for positive/negative tweets? \n",
    "\n",
    "To explore this, let's gather the indices of all positive/negative tweets and calculate the mean tf-idf scores of words appear in each category. \n",
    "\n",
    "We've provided the following starter code to guide you:\n",
    "- Subset the `tweets` dataframe according to the `airline_sentiment` label and retrieve the index of each subset (`.index`). Assign the index to `positive_index` or `negative_index`.\n",
    "- For each subset:\n",
    "    - Retrieve the td-idf representation \n",
    "    - Take the mean tf-idf values across the subset using `.mean()`\n",
    "    - Sort the mean values in the descending order using `.sort_values()`\n",
    "    - Get the top 10 terms using `.head()`\n",
    "\n",
    "Next, run `pos.plot` and `neg.plot` to plot the words with the highest mean tf-idf scores for each subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bfbf838-9ff6-48b8-ad5d-5e75304fe060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the boolean masks \n",
    "positive_index = tweets[tweets['airline_sentiment'] == 'positive'].index # √çndices de tweets con sentimiento positivo\n",
    "negative_index = tweets[tweets['airline_sentiment'] == 'negative'].index # √çndices de tweets con sentimiento negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c67ea1f-de9e-49a9-94f2-a3351446e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following two lines\n",
    "# Obtenerlas palabras m√°s frecuentes en tweets positivos\n",
    "pos = tfidf.loc[positive_index]       # Selecciona los tweets con sentimiento positivo usando sus √≠ndices\n",
    "pos = pos.mean()                      # Calcula el valor promedio TF-IDF de cada palabra en esos tweets\n",
    "pos = pos.sort_values(ascending=False) # Ordena de mayor a menor importancia\n",
    "pos = pos.head(10)   \n",
    "# Obtener las palabras m√°s frecuentes en tweets negativos\n",
    "neg = tfidf.loc[negative_index]       # Selecciona los tweets con sentimiento negativo usando sus √≠ndices\n",
    "neg = neg.mean()                      # Calcula el valor promedio TF-IDF de cada palabra en esos tweets\n",
    "neg = neg.sort_values(ascending=False) # Ordena de mayor a menor importancia\n",
    "neg = neg.head(10)                     # Selecciona las 10 palabras m√°s representativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e29043-8c78-4e41-81d2-b4552030b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las 10 palabras m√°s representativas de tweets positivos usando un gr√°fico de barras horizontal\n",
    "pos.plot(kind='barh', # Tipo de gr√°fico: barras horizontales\n",
    "         xlim=(0, 0.18), # Limitar el eje X entre 0 y 0.18\n",
    "         color='cornflowerblue',  # Color de las barras\n",
    "         title='Top 10 terms with the highest mean tf-idf values for positive tweets');# T√≠tulo del gr√°fico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b25940-2372-4755-818e-f75e4d23daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las 10 palabras m√°s representativas de tweets negativos usando un gr√°fico de barras horizontal\n",
    "neg.plot(kind='barh', # Tipo de gr√°fico: barras horizontales\n",
    "         xlim=(0, 0.18), # Limitar el eje X entre 0 y 0.18\n",
    "         color='darksalmon', # Color de las barras\n",
    "         title='Top 10 terms with the highest mean tf-idf values for negative tweets');# T√≠tulo del gr√°fico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bca876-9649-46f3-bd4f-f9f68fea649a",
   "metadata": {},
   "source": [
    "üîî **Question**: How would you interpret these results? Share your thoughts in the chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da410cb3-a452-441b-a94d-8f751d59d7a6",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "\n",
    "## üé¨ **Demo**: Sentiment Classification Using the TF-IDF Representation\n",
    "\n",
    "Now that we have a tf-idf representation of the text, we are ready to do sentiment analysis!\n",
    "\n",
    "In this demo, we will use a logistic regression model to perform the classification task. Here we briefly step through how logistic regression works as one of the supervised Machine Learning methods, but feel free to explore our workshop on [Python Machine Learning Fundamentals](https://github.com/dlab-berkeley/Python-Machine-Learning) if you want to learn more about it.\n",
    "\n",
    "Logistic regression is a linear model, with which we use to predict the label of a tweet, based on a set of features ($x_1, x_2, x_3, ..., x_i$), as shown below:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "The list of features we'll pass to the model is the vocabulary of the DTM. We also feed the model with a portion of the data, known as the training set, along with other model specification, to learn the coeffient ($\\beta_1, \\beta_2, \\beta_3, ..., \\beta_i$) of each feature. The coefficients tell us whether a feature contributes positively or negatively to the predicted value. The predicted value corresponds to adding all features (multiplied by their coefficients) up, and the predicted value gets passed to a [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) to be converted into the probability space, which tells us whether the predicted label is positive (when $p>0.5$) or negative (when $p<0.5$). \n",
    "\n",
    "The remaining portion of the data, known as the test set, is used to test whether the learned coefficients could be generalized to unseen data. \n",
    "\n",
    "Now that we already have the tf-idf dataframe, the feature set is ready. Let's dive into model specification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33413d63-87eb-489f-b374-3cfeaa51cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV # Importar el modelo de regresi√≥n log√≠stica con validaci√≥n cruzada\n",
    "from sklearn.model_selection import train_test_split # Importar funci√≥n para dividir los datos en conjuntos de entrenamiento y prueba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ff74-3fbb-472a-b795-6f4d18fab215",
   "metadata": {},
   "source": [
    "We'll use the `train_test_split` function from `sklearn` to separate our data into two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cec8b9-14d9-4897-9c02-cc89fcf7b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "# Definir las variables predictoras (features) y la variable objetivo (target)\n",
    "X = tfidf # Variables predictoras: matriz TF-IDF de los tweets\n",
    "y = tweets['airline_sentiment'] # Variable objetivo: sentimiento de cada tweet\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, # Datos y etiquetas\n",
    "    test_size=0.15 # Proporci√≥n del 15% para el conjunto de prueba\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066771d8-2f31-4646-9a1b-6d2b1b9b208c",
   "metadata": {},
   "source": [
    "The `fit_logistic_regression` function is written below to streamline the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46de0b2-af00-4a1d-b4cd-31b96ce545d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una funci√≥n para entrenar un modelo de regresi√≥n log√≠stica\n",
    "def fit_logistic_regression(X, y):\n",
    "    '''Fits a logistic regression model to provided data.'''\n",
    "    # Crear y entrenar el modelo de regresi√≥n log√≠stica con validaci√≥n cruzada\n",
    "    model = LogisticRegressionCV(Cs=10,# N√∫mero de valores de regularizaci√≥n C a probar\n",
    "                                 penalty='l1', # Tipo de penalizaci√≥n L1 (Lasso) para selecci√≥n de variables\n",
    "                                 cv=5,# N√∫mero de pliegues para validaci√≥n cruzada\n",
    "                                 solver='liblinear', # Algoritmo de optimizaci√≥n (compatible con L1)\n",
    "                                 class_weight='balanced',# Ajusta pesos para manejar clases desbalanceadas\n",
    "                                 random_state=42,# Fijar semilla para reproducibilidad\n",
    "                                 refit=True # Reentrena el modelo final usando todos los datos\n",
    "                                 ).fit(X, y)# Ajusta el modelo a los datos proporcionados\n",
    "    return model # Devuelve el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aa7ea-1bc1-43e2-beeb-0ba2da9b2df9",
   "metadata": {},
   "source": [
    "We'll fit the model and compute the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773963bd-6603-4fad-884b-09ce60afab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "model = fit_logistic_regression(X_train, y_train)# Ajusta el modelo de regresi√≥n log√≠stica usando los datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d06c1-d884-45d4-a03d-dd5d40bf70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test accuracy\n",
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")# Muestra la precisi√≥n (accuracy) del modelo en los datos de entrenamiento\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")# Muestra la precisi√≥n (accuracy) del modelo en los datos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e186c5-1719-4deb-bdb4-614a9980f058",
   "metadata": {},
   "source": [
    "The model achieved ~94% accuracy on the training set and ~89% on the test set‚Äîthat's pretty good! The model generalizes reasonably well to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dac39-4753-4ae8-8dfa-e65e5824cccb",
   "metadata": {},
   "source": [
    "Next, let's also take a look at the fitted coefficients to see if what we see makes sense. \n",
    "\n",
    "We can access them using `coef_`, and we can match each coefficient to the tokens from the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb6ef1-13b3-437e-813c-7118911847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefs of all features\n",
    "coefs = model.coef_.ravel()# Obtener los coeficientes (pesos) que el modelo de regresi√≥n log√≠stica aprendi√≥\n",
    "\n",
    "# Get all tokens\n",
    "tokens = vectorizer.get_feature_names_out()# Obtener la lista de tokens (palabras) del vocabulario generado por el vectorizador TF-IDF\n",
    "\n",
    "# Create a token-coef dataframe\n",
    "importance = pd.DataFrame()# Crear un DataFrame vac√≠o para organizar la relaci√≥n entre tokens y coeficientes\n",
    "importance['token'] = tokens # Agregar la columna \"token\" con todas las palabras del vocabulario\n",
    "importance['coefs'] = coefs # Agregar la columna \"coefs\" con los valores de los coeficientes aprendidos para cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63814e-9c0d-4f7a-a5e0-72cca2758d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 tokens with lowest coefs\n",
    "# Ordena el DataFrame 'importance' por la columna 'coefs' en orden ascendente\n",
    "# De esta forma, los coeficientes m√°s negativos (palabras asociadas a sentimientos negativos) quedan arriba\n",
    "neg_coef = importance.sort_values('coefs').head(10)\n",
    "neg_coef # Muestra las primeras 10 filas de ese nuevo DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d596bf7-753c-40cd-ac52-4a37163650ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 tokens with highest coefs\n",
    "# Ordena el DataFrame 'importance' por la columna 'coefs' en orden ascendente\n",
    "# y selecciona las √∫ltimas 10 filas, que corresponden a los coeficientes m√°s altos (positivos)\n",
    "pos_coef = importance.sort_values('coefs').tail(10)\n",
    "pos_coef "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b7893-caa0-4281-98f0-92c9e7b31953",
   "metadata": {},
   "source": [
    "Let's plot the top 10 tokens with the highest/lowest coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1223b-e5c1-4992-bb7e-0a99651c3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.head()\n",
    "pos_coef = importance.sort_values(by='coefs', ascending=True).tail(10)\n",
    "# Plot the top 10 tokens that have the highest coefs\n",
    "# Ordena el DataFrame 'pos_coef' de mayor a menor seg√∫n la columna 'coefs'\n",
    "pos_coef.sort_values('coefs', ascending=False) \\\n",
    "        .plot(kind='barh', # Genera un gr√°fico de barras horizontal (barh) con los datos ordenados\n",
    "              xlim=(0, 18), # Establece el rango del eje X entre 0 y 18\n",
    "              x='token', # Define que el eje Y (categor√≠as) muestre el valor de la columna 'token'\n",
    "              color='cornflowerblue', # Asigna el color 'cornflowerblue' a las barras\n",
    "              title='Top 10 tokens with highest coeffient values'); # Coloca un t√≠tulo al gr√°fico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e00c6-8a9f-484f-aea2-853fd5512083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 tokens that have the lowest coefs\n",
    "neg_coef.plot( \n",
    "                kind='barh',       # Tipo de gr√°fico: barras horizontales\n",
    "                xlim=(0, -18),     # Limite del eje x (aunque negativo aqu√≠ probablemente sea un error)\n",
    "                x='token',         # Define que el eje Y muestre los nombres de los tokens\n",
    "                color='darksalmon',# Color de las barras\n",
    "                title='Top 10 tokens with lowest coeffient values' # T√≠tulo del gr√°fico\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed48ea-fd35-4585-9b98-90456aaee447",
   "metadata": {},
   "source": [
    "Words like \"ruin,\" \"rude,\" and \"hour\" are strong indicators of negative sentiment, while \"thank,\" \"awesome,\" and \"wonderful\" are associated with positive sentiment. \n",
    "\n",
    "We will wrap up Part 2 with these plots. These coefficient terms and the words with the highest TF-IDF values provide different perspectives on the sentiment of tweets. If you'd like, take some time to compare the two sets of plots and see which one provides a better account of the sentiments conveyed in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4430fbd-108f-4a02-ab64-ef36c5949e56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* A Bag-of-Words representation is a simple method to transform our text data to numbers. It focuses on word frequency but not word order. \n",
    "* A TF-IDF representation is a step further; it also considers if a certain word distinctively appears in one document or occurs uniformally across all documents. \n",
    "* With a numerical representation, we can perform a range of text classification task, such as sentiment analysis. \n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
