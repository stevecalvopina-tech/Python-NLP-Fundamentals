{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ffcdc86-76ef-453f-a162-7741c9c23b8c",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Bag of Words\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Learn how to convert text data into numbers through a Bag-of-Words approach.\n",
    "* Understand the TF-IDF algorithm and how it complements the Bag-of-Words representation.\n",
    "* Implement Bag-of-Words and TF-IDF using the `sklearn` package and understand its parameter settings.\n",
    "* Use the numerical representations of text data to perform sentiment analysis.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "🔔 **Question**: A quick question to help you understand what's going on.<br>\n",
    "🥊 **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "🎬 **Demo**: Showing off something more advanced – so you know what Python can be used for!<br> \n",
    "\n",
    "### Sections\n",
    "1. [Exploratory Data Analysis](#section1)\n",
    "2. [Preprocessing](#section2)\n",
    "3. [The Bag-of-Words Representation](#section3)\n",
    "4. [Term Frequency-Inverse Document Frequency](#section4)\n",
    "5. [Sentiment Classification Using the TF-IDF Representation](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e8a36-bd58-4c24-8593-03a0ea70deed",
   "metadata": {},
   "source": [
    "In the previous part, we learned how to perform text preprocessing. However, we didn't move beyond the text data itself. If we're interested in doing any computational analysis on the text data, we still need approaches to convert the text into a **numeric representation**.\n",
    "\n",
    "In Part 2 of our workshop series, we'll explore one of the most straightforward ways to generate a numeric representation from text: the **bag-of-words** (BoW). We will implement the BoW representation to transform our airline tweets data, and then build a classifier to explore what it tells us about the sentiment of the tweets. At the heart of the bag-of-words approach lies the assumption that the frequency of specific tokens is informative about the semantics and sentiment underlying the text.\n",
    "\n",
    "We'll make heavy use of the `scikit-learn` package to do so, as it provides a nice framework for constructing the numeric representation.\n",
    "\n",
    "Let's install `scikit-learn` firstǃ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4a3a0d-66f4-44e5-8dd6-5f441146014d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from scikit-learn) (2.3.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.1-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 1.8/8.7 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.4/8.7 MB 6.7 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.7/8.7 MB 7.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.1/8.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 8.0 MB/s  0:00:01\n",
      "Downloading scipy-1.16.1-cp313-cp313-win_amd64.whl (38.5 MB)\n",
      "   ---------------------------------------- 0.0/38.5 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 2.4/38.5 MB 13.0 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 2.4/38.5 MB 13.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 6.6/38.5 MB 11.0 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 8.9/38.5 MB 11.2 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 10.2/38.5 MB 10.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 10.7/38.5 MB 9.0 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 16.0/38.5 MB 11.1 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 18.4/38.5 MB 11.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 19.7/38.5 MB 11.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 22.3/38.5 MB 10.7 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 23.9/38.5 MB 10.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 26.0/38.5 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 28.3/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 30.4/38.5 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 33.0/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 34.9/38.5 MB 10.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 37.2/38.5 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.5/38.5 MB 10.2 MB/s  0:00:03\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   ------------- -------------------------- 1/3 [scipy]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   -------------------------- ------------- 2/3 [scikit-learn]\n",
      "   ---------------------------------------- 3/3 [scikit-learn]\n",
      "\n",
      "Successfully installed scikit-learn-1.7.1 scipy-1.16.1 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to install the package\n",
    "%pip install scikit-learn #instalar la librería scikit-learn usada para machine learning en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ed437f-9767-43b7-abc5-159aa4339a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: NLTK in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (2025.7.34)\n",
      "Requirement already satisfied: tqdm in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from NLTK) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from click->NLTK) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spaCy in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (0.16.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.32.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.11.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (80.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (25.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.14.1)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (0.4.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spaCy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spaCy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (14.1.0)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spaCy) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy) (1.17.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy) (1.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from jinja2->spaCy) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to install the NLP packages introduced in Part 1\n",
    "%pip install NLTK  # instala NLTK\n",
    "%pip install spaCy # instala spaCy\n",
    "!python -m spacy download en_core_web_sm # descarga el modelo de lenguaje en inglés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3862ffd-918f-4184-8c90-8a39a8a2a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import other packages\n",
    "import re  # Expresiones regulares (para limpieza de texto)\n",
    "import numpy as np # Operaciones numéricas y manejo de arreglos\n",
    "import pandas as pd # Manejo de datos en tablas (DataFrames)\n",
    "import matplotlib.pyplot as plt # Gráficas y visualización básica\n",
    "import seaborn as sns # Visualización avanzada y estética mejorada sobre matplotlib\n",
    "from string import punctuation # Lista de signos de puntuación, útil para limpieza de texto\n",
    "%matplotlib inline # Mostrar gráficos dentro del Jupyter Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852ea4a5-7c28-4557-acdd-afe8a97b7235",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "\n",
    "Before we ever do any preprocessing or modeling, we always should perform exploratory data analysis to familiarize ourselves with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4190e351-97b7-4c5b-866e-07aa6cbd42c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data\n",
    "tweets_path = '../data/airline_tweets.csv' # Ruta donde está guardado el archivo CSV\n",
    "\n",
    "tweets = pd.read_csv(tweets_path, sep=',') # Carga el archivo CSV en un DataFrame de pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79acbaf2-6625-4abb-b50f-97ea54ba0d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head() # Mostra las primeras 5 filas del DataFrame tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80232c78-ac41-4d74-a581-76c9dac3b8f6",
   "metadata": {},
   "source": [
    "As a refresher, each row in this dataframe correponds to a tweet. The following columns are of main interests to us. There are other columns containing metadata of the tweet, such as the author of the tweet, when it was created, the timezone of the user, and others, which we will set aside for now. \n",
    "\n",
    "- `text` (`str`): the text of the tweet.\n",
    "- `airline_sentiment` (`str`): the sentiment of the tweet, labeled as \"neutral,\" \"positive,\" or \"negative.\" \n",
    "- `airline` (`str`): the airline that is tweeted about.\n",
    "- `retweet count` (`int`): how many times the tweet was retweeted.\n",
    "\n",
    "To prepare us for sentiment classification, we'll partition the dataset to focus on the \"positive\" and \"negative\" tweets for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1faaf90-8c01-4d25-9468-90c01823f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra los tweets que no sean 'neutral' en 'airline_sentiment' y reinicia los índices\n",
    "tweets = tweets[tweets['airline_sentiment'] != 'neutral'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb6b039-53e7-4afe-a9e0-b3522c12b2d7",
   "metadata": {},
   "source": [
    "Let's take a look at a few tweets first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "438830e6-1064-47fe-b578-a1ca693a0ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
      "@VirginAmerica and it's a really big bad thing about it\n",
      "@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\n",
      "it's really the only bad thing about flying VA\n",
      "@VirginAmerica yes, nearly every time I fly VX this “ear worm” won’t go away :)\n"
     ]
    }
   ],
   "source": [
    "# Print first five tweets\n",
    "for idx in range(5):\n",
    "    print(tweets['text'].iloc[idx]) # Accede al texto del tweet en la posición 'idx' y lo imprime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6746f8-b29c-40d4-bef6-b4afd4cd6cc1",
   "metadata": {},
   "source": [
    "We can already see that some of these tweets contain negative sentiment—how can we tell this is the case? \n",
    "\n",
    "Next, let's take a look at the distribution of sentiment labels in this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01955158-6954-447a-acb6-2989d02a49c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# Make a bar plot showing the count of tweet sentiments\n",
    "sns.countplot(\n",
    "            data=tweets,  # DataFrame de origen\n",
    "              x='airline_sentiment', # Columna que define las categorías del eje x\n",
    "              color='cornflowerblue', # Color de las barras\n",
    "              order=['positive', 'negative']); # Orden de las categorías en el gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab45abf-adf4-4f5e-ae09-75f6c4fd50d1",
   "metadata": {},
   "source": [
    "It looks like the majority of the tweets in this dataset are expressing negative sentiment!\n",
    "\n",
    "Let's take a look at what gets more retweeted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "428ddde7-af73-4eb6-92c9-041a1791ca59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment\n",
       "negative    0.093375\n",
       "neutral     0.060987\n",
       "positive    0.069403\n",
       "Name: retweet_count, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "tweets_path = '../data/airline_tweets.csv'\n",
    "\n",
    "# Cargar el CSV en un DataFrame llamado 'tweets'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')\n",
    "\n",
    "# Verificar que se cargó correctamente\n",
    "tweets.head()\n",
    "\n",
    "# Get the mean retweet count for each sentiment\n",
    "# Agrupa los tweets por la columna 'airline_sentiment'. Luego selecciona la columna 'retweet_count' y calcula la media (promedio) de cada grupo\n",
    "tweets.groupby('airline_sentiment')['retweet_count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d31f3bc-257c-48a8-86a0-fd0d7c3e8cb3",
   "metadata": {},
   "source": [
    "Negative tweets are clearly retweeted more often than positive ones!\n",
    "\n",
    "Let's see which airline receives most negative tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12aa9f2d-d655-494a-bb72-08ad973518f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>airline</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>US Airways</th>\n",
       "      <td>0.776862</td>\n",
       "      <td>0.130793</td>\n",
       "      <td>0.092345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>American</th>\n",
       "      <td>0.710402</td>\n",
       "      <td>0.167814</td>\n",
       "      <td>0.121783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United</th>\n",
       "      <td>0.688906</td>\n",
       "      <td>0.182365</td>\n",
       "      <td>0.128728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Southwest</th>\n",
       "      <td>0.490083</td>\n",
       "      <td>0.274380</td>\n",
       "      <td>0.235537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Delta</th>\n",
       "      <td>0.429793</td>\n",
       "      <td>0.325383</td>\n",
       "      <td>0.244824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Virgin America</th>\n",
       "      <td>0.359127</td>\n",
       "      <td>0.339286</td>\n",
       "      <td>0.301587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "airline_sentiment  negative   neutral  positive\n",
       "airline                                        \n",
       "US Airways         0.776862  0.130793  0.092345\n",
       "American           0.710402  0.167814  0.121783\n",
       "United             0.688906  0.182365  0.128728\n",
       "Southwest          0.490083  0.274380  0.235537\n",
       "Delta              0.429793  0.325383  0.244824\n",
       "Virgin America     0.359127  0.339286  0.301587"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the proportion of negative tweets by airline\n",
    "# 1. Agrupa por 'airline' y 'airline_sentiment' y cuenta el número de tweets por grupo\n",
    "# 2. Divide por el total de tweets de cada aerolínea\n",
    "proportions = tweets.groupby(['airline', 'airline_sentiment']).size() / tweets.groupby('airline').size()\n",
    "proportions.unstack().sort_values('negative', ascending=False) # Reorganiza los datos en formato tabla (sentimientos como columnas) y ordena por la proporción de negativos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7042419e-9c41-40e7-8dbf-47bd1e2ad45a",
   "metadata": {},
   "source": [
    "It looks like people are most dissatified with US Airways, followed by American Airline, both having over 85\\% negative tweets!\n",
    "\n",
    "A lot of interesting discoveries could be made if you want to explore more about the data. Now let's return to our task of sentiment analysis. Before that, we need to preprocess the text data so that they are in a standard format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e513930-2dc7-489c-bc5a-22eb09add5bf",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "# Preprocessing\n",
    "\n",
    "We spent much of Part 1 learning how to preprocess data. Let's apply what we learned! Looking at some of the tweets above, we can see that while they are in pretty good shape, we can do some additional processing on them.\n",
    "\n",
    "In our pipeline, we'll omit the tokenization process since we will perform it in a later step. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a83ece-f3b2-4200-9d22-0788fbc07fa4",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 1: Apply a Text Cleaning Pipeline\n",
    "\n",
    "Write a function called `preprocess` that performs the following steps on a text input:\n",
    "\n",
    "* Step 1: Lowercase the text input.\n",
    "* Step 2: Replace the following patterns with placeholders:\n",
    "    * URLs &rarr; ` URL `\n",
    "    * Digits &rarr; ` DIGIT `\n",
    "    * Hashtags &rarr; ` HASHTAG `\n",
    "    * Tweet handles &rarr; ` USER `\n",
    "* Step 3: Remove extra blankspace.\n",
    "\n",
    "Here are some hints to guide you through this challenge:\n",
    "\n",
    "* For Step 1, recall from Part 1 that a string method called [`.lower()`](https://docs.python.org/3.11/library/stdtypes.html#str.lower) can be usd to convert text to lowercase. \n",
    "* We have integrated Step 2 into a function called `placeholder`. Run the cell below to import it into your notebook, and you can use it just like any other functions.\n",
    "* For Step 3, we have provided the regex pattern for identifying whitespace characters as well as the correct replacement for extract whitespace. \n",
    "\n",
    "Run your `preprocess` function on `example_tweet` (three cells below) to check if it works. If it does, apply it to the entire `text` column in the tweets dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21738b02-9ab9-4a61-b41f-ff75888aa747",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\geoco\\OneDrive\\Escritorio\\UIDE\\Practicas\\Analisis_datos\\Python-NLP-Fundamentals\\lessons\\utils.py:4: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  digit_pattern = '\\d+'\n",
      "c:\\Users\\geoco\\OneDrive\\Escritorio\\UIDE\\Practicas\\Analisis_datos\\Python-NLP-Fundamentals\\lessons\\utils.py:14: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  digit_pattern = '\\d+'\n"
     ]
    }
   ],
   "source": [
    "from utils import placeholder #importa una función o variable llamada placeholder desde un módulo llamado utils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03569f0d-34ba-492d-aa1d-1dce9d34f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "blankspace_pattern = r'\\s+'# Patrón de expresión regular: uno o más espacios en blanco consecutivos\n",
    "blankspace_repl = ' ' # Reemplazo: un solo espacio\n",
    "\n",
    "def preprocess(text):\n",
    "    '''Create a preprocess pipeline that cleans the tweet data.'''\n",
    "    \n",
    "    # Step 1: Lowercase\n",
    "    text = text.lower()    # Paso 1: Convertir todo el texto a minúsculas\n",
    "\n",
    "\n",
    "    # Step 2: Replace patterns with placeholders\n",
    "    text = re.sub(pattern=blankspace_pattern, repl=blankspace_repl, string=text)   # Usamos la función re.sub para reemplazar todos los espacios múltiples por un solo espacio\n",
    "\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = text.strip()    # Paso 3: Eliminar espacios en blanco al inicio y final del texto\n",
    "\n",
    "\n",
    "    return text     # Devolver el texto limpio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8990cefd-5d04-46ba-ada2-29978c28cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\n",
      "==================================================\n",
      "lol @justinbeiber and @billgates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo\n"
     ]
    }
   ],
   "source": [
    "import re  # Librería para expresiones regulares\n",
    "\n",
    "# Define un tweet de ejemplo con menciones, hashtags, URL y números\n",
    "example_tweet = 'lol @justinbeiber and @BillGates are like soo 2000 #yesterday #amiright saw it on https://twitter.com #yolo'\n",
    "\n",
    "# Print the example tweet\n",
    "print(example_tweet) # Imprime el tweet original\n",
    "print(f\"{'='*50}\") # Imprime una línea separadora\n",
    "\n",
    "# Print the preprocessed tweet\n",
    "print(preprocess(example_tweet))# Esto convierte a minúsculas, reemplaza espacios múltiples por uno solo y elimina espacios al inicio y final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f7bb6a-f064-48cc-b650-12c4ef2fbb88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                  @virginamerica what @dhepburn said.\n",
       "1    @virginamerica plus you've added commercials t...\n",
       "2    @virginamerica i didn't today... must mean i n...\n",
       "3    @virginamerica it's really aggressive to blast...\n",
       "4    @virginamerica and it's a really big bad thing...\n",
       "Name: text_processed, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the text column and assign the preprocessed tweets to a new column\n",
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))# Guarda los tweets limpios en una nueva columna llamada 'text_processed'\n",
    "\n",
    "tweets['text_processed'].head()# Muestra los primeros cinco tweets preprocesados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1576acc6-b305-492a-8fde-65b343cb779c",
   "metadata": {},
   "source": [
    "Congratualtions! Preprocessing is done. Let's dive into the bag-of-words!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53282330-54da-4e1c-bfe5-e77cb8fa3add",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "# The Bag-of-Words Representation\n",
    "\n",
    "The idea of bag-of-words (BoW), as the name suggests, is quite intuitive: we take a document and toss it in a bag. The action of \"throwing\" the document in a bag disregards the relative position between words, so what is \"in the bag\" is essentially \"an unsorted set of words\" [(Jurafsky & Martin, 2024)](https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf). In return, we have a list of unique words and the frequency of each of them. \n",
    "\n",
    "For example, as shown in the following illustration, the word \"coffee\" appears twice. \n",
    "\n",
    "<img src='../images/bow-illustration-1.png' alt=\"BoW-Part2\" width=\"600\">\n",
    "\n",
    "With a bag-of-words representation, we make heavy use of word frequency but not too much of word order. \n",
    "\n",
    "In the context of sentiment analysis, the sentiment of a tweet is conveyed more strongly by specific words. For example, if a tweet contains the word \"happy,\" it likely conveys positive sentiment, but not always (e.g., \"not happy\" denotes the opposite sentiment). When these words come up more often, they'll probably more strongly convey the sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d9bdbd-406d-469b-a8f6-41d1b3687c37",
   "metadata": {},
   "source": [
    "## Document Term Matrix\n",
    "\n",
    "Now let's implement the idea of bag-of-words. Before we dive deeper, let's step back for a moment. In practice, text analysis often involves handling many documents; from now on, we use the term **document** to represent a piece of text on which we perform analysis. It could be a phrase, a sentence, a tweet, or any other text—as long as it can be represented by a string, the length dosen't really matter. \n",
    "\n",
    "Imagine we have four documents (i.e., the four phrases shown above), and we toss them all in the bag. Instead of a word-frequency list, we'd expect a document-term matrix (DTM) in return. In a DTM, the word list is the **vocabulary** (V) that holds all unique words occur across the documents. For each **document** (D), we count the number of occurence of each word in the vocabulary, and then plug the number into the matrix. In other words, the DTM we will construct is a $D \\times V$ matrix, where each row corresponds to a document, and each column corresponds to a token (or \"term\").\n",
    "\n",
    "The unique tokens in this set of documents, arranged in alphabetical order, form the columns. For each document, we mark the occurence of each word present in the document. The numerical representation for each document is a row in the matrix. For example, the first document, \"the coffee roaster,\" has the numerical representation $[0, 1, 0, 0, 0, 1, 1, 0]$.\n",
    "\n",
    "Note that the left index column now displays these documents as text, but typically we would just assign an index to each of them. \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|cccccccccccc}\n",
    " & \\text{americano} & \\text{coffee} & \\text{iced} & \\text{light} & \\text{roast} & \\text{roaster} & \\text{the} & \\text{time} \\\\\\hline\n",
    "\\text{the coffee roaster} &0 &1\t&0\t&0\t&0\t&1\t&1\t&0 \\\\ \n",
    "\\text{light roast} &0 &0\t&0\t&1\t&1\t&0\t&0\t&0 \\\\\n",
    "\\text{iced americano} &1 &0\t&1\t&0\t&0\t&0\t&0\t&0 \\\\\n",
    "\\text{coffee time} &0 &1\t&0\t&0\t&0\t&0\t&0\t&1 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "To create a DTM, we will use `CountVectorizer` from the package `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd2adf56-ba93-459d-8cfa-16ce8dc9284b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer# Importa CountVectorizer de scikit-learn, que convierte texto en una matriz de conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4989781d-6b40-417a-be70-eeba05cd8a50",
   "metadata": {},
   "source": [
    "The following illustration depicts the three-step workflow of creating a DTM with `CountVectorizr`.\n",
    "\n",
    "<img src='../images/CountVectorizer1.png' alt=\"CountVectorizer\" width=\"500\">\n",
    "\n",
    "Let's walk through these steps with the toy example shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34174034-46b9-43e2-a511-5972d378cb00",
   "metadata": {},
   "source": [
    "### A Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4da2bd3d-0460-4b5f-9b9e-02940db0d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy example containing four documents\n",
    "test = ['the coffee roaster',# Documento 1: contiene las palabras 'the', 'coffee', 'roaster'\n",
    "        'light roast',  # Documento 2: contiene las palabras 'light', 'roast'\n",
    "        'iced americano', # Documento 3: contiene las palabras 'iced', 'americano'\n",
    "        'coffee time']  # Documento 4: contiene las palabras 'coffee', 'time'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c1d3-fcee-4e20-b9a7-17306ebd5fc2",
   "metadata": {},
   "source": [
    "The first step is to initialize a `CountVectorizer` object. Within the round paratheses, we can specify parameter settings if desired. Let's take a look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and see what options are available.  \n",
    "\n",
    "For now we can just leave it blank to use the default settings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9de3fe6a-9abf-4e11-aad1-e54c891567bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer() # Este objeto se utilizará para transformar la lista de documentos de texto en una matriz de conteo de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5a7d0d-0bfc-4fb9-8e5f-e91e39797fb5",
   "metadata": {},
   "source": [
    "The second step is to `fit` this `CountVectorizer` object to the data, which means creating a vocabulary of tokens from the set of documents. Thirdly, we `transform` our data according to the \"fitted\" `CountVectorizer` object, which means taking each of the document and counting the occurrences of tokens according to the vocabulary established during the \"fitting\" step.\n",
    "\n",
    "It may sound a bit complex but steps 2 and 3 can be done in one swoop using a `fit_transform` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da1bbad4-bb1a-4b92-9096-6e17558b4a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform to create a DTM\n",
    "test_count = vectorizer.fit_transform(test) # Ajusta (fit) el CountVectorizer al corpus y transforma (transform) los documentos en una matriz de conteo de palabras (DTM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324d3b65-4e98-48bf-87d2-399457f4939c",
   "metadata": {},
   "source": [
    "The return of `fit_transform` is supposed to be the DTM. \n",
    "\n",
    "Let's take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb044001-8eb2-4489-b025-2d8e2d4bfee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 9 stored elements and shape (4, 8)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_count #Es la matriz dispersa que resulta de aplicar CountVectorizer a la lista test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9817b09-a806-42c4-9436-822cc27a38b9",
   "metadata": {},
   "source": [
    "Apparently we've got a \"sparse matrix\"—a matrix that contains a lot of zeros. This makes sense. For each document, there are words that don't occur at all, and these are counted as zero in the DTM. This sparse matrix is stored in a \"Compressed Sparse Row\" format, a memory-saving format designed for handling sparse matrices. \n",
    "\n",
    "Let's convert it to a dense matrix, where those zeros are probably represented, as in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03a238-87d8-40c9-b20e-66e7c9b6576b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert DTM to a dense matrix \n",
    "test_count_dense = test_count.todense() # Convierte la matriz dispersa (DTM) en una matriz densa para poder visualizar los conteos de palabras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b58a63-d7f6-4b9f-aadf-4d4fc7341336",
   "metadata": {},
   "source": [
    "So this is our DTM! The matrix is the same as shown above. To make it more reader-friendly, let's convert it to a dataframe. The column names should be tokens in the vocabulary, which we can access with the `get_feature_names_out` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "714de5d3-e37d-4a19-9ade-3c6629e38d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['americano', 'coffee', 'iced', 'light', 'roast', 'roaster', 'the',\n",
       "       'time'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the vocabulary\n",
    "vectorizer.get_feature_names_out()# Devuelve un array con todas las palabras únicas de los documentos en orden alfabético\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a7729a2-ca2e-4de7-8795-74dfedb7a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DTM dataframe\n",
    "# Cada fila representa un documento\n",
    "# Cada columna representa una palabra del vocabulario\n",
    "# Los valores indican cuántas veces aparece cada palabra en cada documento\n",
    "test_dtm = pd.DataFrame(data=test_count.todense(), # Convertimos la matriz dispersa a una matriz densa\n",
    "                        columns=vectorizer.get_feature_names_out()) # Nombramos las columnas con las palabras del vocabulario\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781da407-f394-40f2-9d45-1fac39f02047",
   "metadata": {},
   "source": [
    "Here it is! The DTM of our toy data is now a dataframe. The index of `test_dtm` corresponds to the position of each document in the `test` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e41dd243-cd2e-43c3-80f8-5eaab6e64210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>americano</th>\n",
       "      <th>coffee</th>\n",
       "      <th>iced</th>\n",
       "      <th>light</th>\n",
       "      <th>roast</th>\n",
       "      <th>roaster</th>\n",
       "      <th>the</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   americano  coffee  iced  light  roast  roaster  the  time\n",
       "0          0       1     0      0      0        1    1     0\n",
       "1          0       0     0      1      1        0    0     0\n",
       "2          1       0     1      0      0        0    0     0\n",
       "3          0       1     0      0      0        0    0     1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm # DataFrame que representa la Document-Term Matrix (DTM) de los documentos de ejemplo 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59a03b4-94fa-4fe7-8f5d-7280e31b9bc4",
   "metadata": {},
   "source": [
    "Hopefully this toy example provides a clear walkthrough of creating a DTM.\n",
    "\n",
    "Now it's time for our tweets data!\n",
    "\n",
    "### DTM for Tweets\n",
    "\n",
    "We'll begin by initializing a `CountVectorizer` object. In the following cell, we have included a few parameters that people often adjust. These parameters are currently set to their default values.\n",
    "\n",
    "When we construct a DTM, the default is to lowercase the input text. If nothing is provided for `stop_words`, the default is to keep them. The next three parameters are used to control the size of the vocabulary, which we'll return to in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "783e44a4-4a22-4290-b222-282b02c080dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer(lowercase=True,# Convierte todo el texto a minúsculas antes de tokenizar\n",
    "                             stop_words=None, # No elimina ninguna palabra por defecto (no usa stopwords)\n",
    "                             min_df=1, # La palabra debe aparecer al menos en 1 documento para incluirse en el vocabulario\n",
    "                             max_df=1.0, # La palabra puede aparecer como máximo en el 100% de los documentos\n",
    "                             max_features=None)  # No hay límite en el número de características (palabras) a extraer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f85e76ea-bc54-4775-bcda-432a03d2c96f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'int64'\n",
       "\twith 234281 stored elements and shape (14640, 15051)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform to create DTM\n",
    "# Ajusta (fit) el CountVectorizer al corpus de tweets preprocesados y transforma (transform) cada tweet en un vector de conteo de palabras (DTM)\n",
    "counts = vectorizer.fit_transform(tweets['text_processed'])\n",
    "counts # counts ahora contiene la Document-Term Matrix (DTM) en formato de matriz dispersa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87119057-c78c-4eb2-a9d6-3e9f44e4c22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not run if you have limited memory - this includes DataHub and Binder\n",
    "np.array(counts.todense()) # Convierte la matriz dispersa 'counts' a una matriz densa de NumPy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99322b85-1a15-46a5-bb80-bb5eaa6eeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract tokens\n",
    "tokens = vectorizer.get_feature_names_out() # Estos tokens corresponden a las columnas de la Document-Term Matrix (DTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43620587-3795-4434-8f1f-145c81b93706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DTM\n",
    "# Cada fila corresponde a un tweet (usando el mismo índice que el DataFrame original)\n",
    "# Cada columna corresponde a una palabra del vocabulario\n",
    "first_dtm = pd.DataFrame(data=counts.todense(), # Convertimos la matriz dispersa a matriz densa\n",
    "                         index=tweets.index, # Usamos los índices originales de los tweets\n",
    "                         columns=tokens)  # Nombres de columnas = palabras únicas aprendidas por CountVectorizer\n",
    "\n",
    "# Print the shape of DTM\n",
    "print(first_dtm.shape) # Muestra (cantidad de tweets, cantidad de palabras únicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd257d5-4244-436c-afe7-5688232caf8f",
   "metadata": {},
   "source": [
    "If we leave the `CountVectorizer` to the default setting, the vocabulary size of the tweet data is 8751. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3604ec-d909-4238-9a3f-67e7d4ae2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dtm.head() # Muestra las primeras 5 filas del DataFrame `first_dtm`\n",
    "                   # Cada fila = un tweet\n",
    "                   # Cada columna = una palabra del vocabulario\n",
    "                   # Cada valor = número de veces que aparece esa palabra en ese tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095d34e2-52f8-4419-b4c7-ed20dbd5df89",
   "metadata": {},
   "source": [
    "Most of the tokens have zero occurences at least in the first five tweets. \n",
    "\n",
    "Let's take a closer look at the DTM!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f432154a-eae0-4723-a797-55f3cfdd71c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequent tokens\n",
    "# 1. first_dtm.sum() → suma los valores de cada columna, es decir, cuenta cuántas veces aparece cada palabra en todos los tweets\n",
    "# 2. .sort_values(ascending=False) → ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) → muestra las 10 palabras más frecuentes\n",
    "first_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7f1c9-dd66-49f2-b337-01253da551d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_exact_                     1\n",
       "mightmismybrosgraduation    1\n",
       "midterm                     1\n",
       "midnite                     1\n",
       "midland                     1\n",
       "michelle                    1\n",
       "michele                     1\n",
       "michael                     1\n",
       "mhtt                        1\n",
       "mgmt                        1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Least frequent tokens\n",
    "# 1. first_dtm.sum() → suma los valores de cada columna, es decir, cuenta cuántas veces aparece cada palabra en todos los tweets\n",
    "# 2. .sort_values(ascending=True) → ordena las palabras de menor a mayor frecuencia\n",
    "# 3. .head(10) → muestra las 10 palabras menos frecuentes\n",
    "first_dtm.sum().sort_values(ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d230f79-e752-4e32-93db-4f013287f8e2",
   "metadata": {},
   "source": [
    "It is not surprising to see \"user\" and \"digit\" to be among the most frequent tokens as we replaced each idiosyncratic one with these placeholders. The rest of the most frequent tokens are mostly stop words.\n",
    "\n",
    "Perhaps a more interesting pattern is to look for which token appears most in any given tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb8f4d8-4c88-4155-a6c5-c72a5b4e8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.DataFrame() # Crear un DataFrame vacío para almacenar los tokens más frecuentes por tweet\n",
    "\n",
    "# Retrieve the index of the tweet where a token appears most frequently\n",
    "counts['token'] = first_dtm.idxmax(axis=1)# idxmax(axis=1) devuelve el nombre de la columna con el valor máximo por fila\n",
    "\n",
    "# Retrieve the number of occurrence\n",
    "# Recuperar el nombre del token que aparece con mayor frecuencia en cada tweet \n",
    "counts['number'] = first_dtm.max(axis=1) # max(axis=1) devuelve el valor máximo por fila (frecuencia del token)\n",
    "\n",
    "# Filter out placeholders\n",
    "# Filtrar los tokens que son placeholders (digit, hashtag, user)\n",
    "# y mostrar los 10 tokens más frecuentes después del filtrado\n",
    "counts[(counts['token']!='digit')\n",
    "       & (counts['token']!='hashtag')\n",
    "       & (counts['token']!='user')].sort_values('number', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdac4ef-6b9d-4aad-9b24-c70f6c2eb8f0",
   "metadata": {},
   "source": [
    "It looks like among all tweets, at most a token appears six times, and it is either the word \"It\" or the word \"worst.\" \n",
    "\n",
    "Let's go back to our tweets dataframe and locate the 918th tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5e7cacd8-1fb3-4f0d-a744-4ee0994a089f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@united Agent in LAS letting 20 customers know they can't help them rebook delayed flight to DEN #unfriendlyskies http://t.co/QuzVmK2rTR\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve 918th tweet: \"worst\"\n",
    "# Recuperar el tweet en la posición 918 del DataFrame\n",
    "# Muestra el contenido del tweet original en la columna 'text'\n",
    "tweets.iloc[918]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dba8e37-4880-4565-b6fc-7e7c96958f0f",
   "metadata": {},
   "source": [
    "## Customize the `CountVectorizer`\n",
    "\n",
    "So far we've always used the default parameter setting to create our DTMs, but in many cases we may want to customize the `CountVectorizer` object. The purpose of doing so is to further filter out unnecessary tokens. In the example below, we tweak the following parameters:\n",
    "\n",
    "- `stop_words = 'english'`: ignore English stop words \n",
    "- `min_df = 2`: ignore words that don't occur at least twice\n",
    "- `max_df = 0.95`: ignore words if they appear in more than 95\\% of the documents\n",
    "\n",
    "🔔 **Question**: Let's pause for a minute to discuss whether it sounds reasonable to set these parameters! What do you think?\n",
    "\n",
    "Oftentimes, we are not interested in words whose frequencies are either too low or too high, so we use `min_df` and `max_df` to filter them out. Alternatively, we can define our vocabulary size as $N$ by setting `max_features`. In other words, we tell `CountVectorizer` to only consider the top $N$ most frequent tokens when constructing the DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37a0a93e-9dd8-43dc-a82c-06a24bf02bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the parameter setting\n",
    "# Crear un objeto CountVectorizer con parámetros personalizados\n",
    "vectorizer = CountVectorizer(lowercase=True, # Convierte todo el texto a minúsculas antes de tokenizar\n",
    "                             stop_words='english', # Elimina las palabras vacías (stop words)\n",
    "                             min_df=2,  # Ignora palabras que aparecen en menos de 2 documentos\n",
    "                             max_df=0.95, # Ignora palabras que aparecen en más del 95% de los documentos\n",
    "                             max_features=None) # No limita el número máximo de palabras (todas se incluyen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b53e5ecf-7be3-4915-9d11-fd3edb913400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit, transform, and get tokens\n",
    "# Ajustar el vectorizador al texto preprocesado y transformar los tweets en una matriz de conteo\n",
    "counts = vectorizer.fit_transform(tweets['text_processed']) # Devuelve una matriz dispersa donde cada fila = tweet, cada columna = palabra, y cada celda = número de apariciones\n",
    "tokens = vectorizer.get_feature_names_out() # Obtener los tokens (palabras únicas) identificadas por el CountVectorizer\n",
    "\n",
    "# Create the second DTM\n",
    "# Crear un DataFrame de la segunda Document-Term Matrix (DTM)\n",
    "second_dtm = pd.DataFrame(data=counts.todense(), # Convertir la matriz dispersa a matriz densa\n",
    "                          index=tweets.index,  # Mantener los índices originales de los tweets\n",
    "                          columns=tokens) # Usar los tokens como nombres de columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e66bc-2eaa-4642-8848-74459948084b",
   "metadata": {},
   "source": [
    "Our second DTM has a substantially smaller vocabulary compared to the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570fb598-fa81-4111-9e36-7172d8034713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la forma (número de filas y columnas) de la primera DTM\n",
    "# Cada fila = un tweet, cada columna = una palabra\n",
    "print(first_dtm.shape)\n",
    "# Imprimir la forma (número de filas y columnas) de la segunda DTM\n",
    "# Esta DTM usa parámetros de CountVectorizer más estrictos (stop words, min_df, max_df)\n",
    "print(second_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d8deabb2-20eb-4047-b592-48cb1564fd2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>0510</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>...</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zik2uoxgnw</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6144 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00am  00pm  02  03  05  0510  05am  05pm  ...  yvonne  yvr  yyj  \\\n",
       "0   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "1   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "2   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "3   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "4   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "\n",
       "   yyz  zero  zik2uoxgnw  zkatcher  zone  zoom  zurich  \n",
       "0    0     0           0         0     0     0       0  \n",
       "1    0     0           0         0     0     0       0  \n",
       "2    0     0           0         0     0     0       0  \n",
       "3    0     0           0         0     0     0       0  \n",
       "4    0     0           0         0     0     0       0  \n",
       "\n",
       "[5 rows x 6144 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Muestra las primeras 5 filas del DataFrame `second_dtm`\n",
    "# Cada fila = un tweet\n",
    "# Cada columna = una palabra del vocabulario filtrado por CountVectorizer\n",
    "# Cada valor = número de veces que aparece esa palabra en ese tweet\n",
    "second_dtm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998fe2c3-ec90-4027-8c7f-417327a33a27",
   "metadata": {},
   "source": [
    "The most frequent token list now includes words that make more sense to us, such as \"cancelled\" and \"service.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ffa7bf4e-640b-49bc-b64b-721140f67f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "united          4164\n",
       "flight          3939\n",
       "usairways       3053\n",
       "americanair     2964\n",
       "southwestair    2461\n",
       "jetblue         2395\n",
       "http            1155\n",
       "thanks          1083\n",
       "cancelled       1065\n",
       "just             974\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las 10 palabras más frecuentes en la segunda DTM\n",
    "# 1. second_dtm.sum() → suma los valores de cada columna, contando cuántas veces aparece cada palabra en todos los tweets\n",
    "# 2. .sort_values(ascending=False) → ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) → muestra las 10 palabras más frecuentes\n",
    "second_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b5145-d505-4e36-9a39-a40d25d8ec6f",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 2: Lemmatize the Text Input\n",
    "\n",
    "Recall from Part 1 that we introduced using `spaCy` to perform lemmatization, i.e., to \"recover\" the base form of a word. This process will reduce vocabulary size by keeping word variations minimal—a smaller vocabularly may help improve model performance in sentiment classification. \n",
    "\n",
    "Now let's implement lemmatization on our tweet data and use the lemmatized text to create a third DTM. \n",
    "\n",
    "Complete the function `lemmatize_text`. It requires a text input and returns the lemmas of all tokens. \n",
    "\n",
    "Here are some hints to guide you through this challenge:\n",
    "\n",
    "- Step 1: initialize a list to hold lemmas\n",
    "- Step 2: apply the `nlp` pipeline to the input text\n",
    "- Step 3: iterate over tokens in the processed text and retrieve the lemma of the token\n",
    "    - HINT: lemmatization is one of the linguistic annotations that the `nlp` pipeline automatically does for us. We can use `token.lemma_` to access the annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "da610560-62c3-48ab-a1b2-25e0b589bc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import spaCy\n",
    "import spacy # Importa la librería spaCy para procesamiento de lenguaje natural\n",
    "nlp = spacy.load('en_core_web_sm') # Carga el modelo en inglés pequeño 'en_core_web_sm' para tokenización, lematización y reconocimiento de entidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98ead266-30f3-48ad-bc51-c1685487f000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    '''Lemmatize the text input with spaCy annotations.'''\n",
    "\n",
    "    # Step 1: Initialize an empty list to hold lemmas\n",
    "    lemma = [] \n",
    "\n",
    "    # Step 2: Apply the nlp pipeline to input text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Step 3: Iterate over tokens in the text to get the token lemma\n",
    "    for token in doc:\n",
    "        lemma.append(token.lemma_)# token.lemma_ devuelve la forma base del token\n",
    "\n",
    "    # Step 4: Join lemmas together into a single string\n",
    "    text_lemma = ' '.join(lemma)\n",
    "    \n",
    "    return text_lemma # Devolver el texto lematizado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf36aab6-35dd-42a2-9b38-b7c432f021c6",
   "metadata": {},
   "source": [
    "Let's apply the function to the following example tweet first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "742e82bb-5c42-4fa8-9101-5a0ea908db25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@virginamerica awaiting my return phone call, just would prefer to use your online self-service option :(\n",
      "==================================================\n",
      "@virginamerica await my return phone call , just would prefer to use your online self - service option :(\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to an example tweet\n",
    "print(tweets.iloc[33][\"text_processed\"])# Mostra el tweet preprocesado en la posición 33\n",
    "print(f\"{'='*50}\") # Imprime una línea separadora\n",
    "# Mostrar el mismo tweet después de aplicar la función de lematización\n",
    "# Cada palabra se convierte a su forma base (por ejemplo: \"running\" → \"run\")\n",
    "print(lemmatize_text(tweets.iloc[33]['text_processed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeda987-dc32-4979-b158-c24be7d1a420",
   "metadata": {},
   "source": [
    "And then let's lemmatize the tweet data and save the output to a new column `text_lemmatized`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1ac128d2-1be5-4ef5-bb50-5b8d44ef8ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a while!\n",
    "tweets['text_lemmatized'] = tweets['text_processed'].apply(lambda x: lemmatize_text(x))# Aplica la función de lematización a todos los tweets preprocesados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c02aad6-4e71-4afc-80cf-31d4f39498b2",
   "metadata": {},
   "source": [
    "Now with the `text_lemmatized` column, let's create a third DTM. The parameter setting is the same as the second DTM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5f49d790-3c9d-4dc1-a5c9-72c306630412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>0510</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>...</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zik2uoxgnw</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  00am  00pm  02  03  05  0510  05am  05pm  ...  yvonne  yvr  yyj  \\\n",
       "0   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "1   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "2   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "3   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "4   0    0     0     0   0   0   0     0     0     0  ...       0    0    0   \n",
       "\n",
       "   yyz  zero  zik2uoxgnw  zkatcher  zone  zoom  zurich  \n",
       "0    0     0           0         0     0     0       0  \n",
       "1    0     0           0         0     0     0       0  \n",
       "2    0     0           0         0     0     0       0  \n",
       "3    0     0           0         0     0     0       0  \n",
       "4    0     0           0         0     0     0       0  \n",
       "\n",
       "[5 rows x 5090 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the vectorizer (the same param setting as previous)\n",
    "vectorizer = CountVectorizer(lowercase=True,# Convierte todo el texto a minúsculas\n",
    "                             stop_words='english', # Eliminar stop words en inglés\n",
    "                             min_df=2, # Ignora palabras que aparecen en menos de 2 documentos\n",
    "                             max_df=0.95, # Ignora palabras que aparecen en más del 95% de los documentos\n",
    "                             max_features=None) # No limita el número de palabras\n",
    "\n",
    "# Fit, transform, and get tokens\n",
    "counts = vectorizer.fit_transform(tweets['text_lemmatized']) # Ajusta el vectorizador a los tweets lematizados y transformar en matriz de conteo\n",
    "tokens = vectorizer.get_feature_names_out() # Obtiene los tokens (palabras únicas) identificadas por CountVectorizer\n",
    "\n",
    "# Create the third DTM\n",
    "third_dtm = pd.DataFrame(data=counts.todense(), # Convierte la matriz dispersa a matriz densa\n",
    "                         index=tweets.index, # Mantene los índices originales de los tweets\n",
    "                         columns=tokens) # Usa los tokens como nombres de columnas\n",
    "third_dtm.head() # Mostrar las primeras 5 filas de la tercera DTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9859eb04-dbd2-4fa0-9798-65ed7496c297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shapes of three DTMs\n",
    "# Imprime las dimensiones de las tres Document-Term Matrices (DTM)\n",
    "# Cada fila = número de tweets\n",
    "# Cada columna = número de palabras únicas en el vocabulario correspondiente\n",
    "print(first_dtm.shape)   # Primera DTM: sin filtrar stop words ni aplicar frecuencia mínima/máxima\n",
    "print(second_dtm.shape)  # Segunda DTM: con stop words y filtros de frecuencia aplicados\n",
    "print(third_dtm.shape)   # Tercera DTM: después de lematización y filtros, más refinada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa94c8ac-e4f4-4b76-afdb-1d4af54a3eee",
   "metadata": {},
   "source": [
    "Let's print the top 10 most frequent tokens as usual. These tokens are now lemmas and their counts also change after lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5745ca29-97ed-4fe1-81db-7e402c8da674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flight          4754\n",
       "americanair     2964\n",
       "united          2591\n",
       "southwestair    2461\n",
       "jetblue         2395\n",
       "usairway        2366\n",
       "thank           1680\n",
       "unite           1574\n",
       "http            1155\n",
       "hour            1152\n",
       "dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most frequent tokens in the third DTM\n",
    "# 1. third_dtm.sum() → suma los valores de cada columna, contando cuántas veces aparece cada palabra en todos los tweets lematizados\n",
    "# 2. .sort_values(ascending=False) → ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) → muestra las 10 palabras más frecuentes\n",
    "third_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "16c63e6a-50c3-448a-9a56-a1d193cd6680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "united          4164\n",
       "flight          3939\n",
       "usairways       3053\n",
       "americanair     2964\n",
       "southwestair    2461\n",
       "jetblue         2395\n",
       "http            1155\n",
       "thanks          1083\n",
       "cancelled       1065\n",
       "just             974\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compared to the most frequent tokens in the second DTM\n",
    "# 1. second_dtm.sum() → suma los valores de cada columna, contando cuántas veces aparece cada palabra en todos los tweets preprocesados (sin lematización)\n",
    "# 2. .sort_values(ascending=False) → ordena las palabras de mayor a menor frecuencia\n",
    "# 3. .head(10) → muestra las 10 palabras más frecuentes\n",
    "second_dtm.sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38363398-fdf5-456b-ae3d-cae9d5294140",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "\n",
    "# Term Frequency-Inverse Document Frequency \n",
    "\n",
    "So far, we're relying on word frequency to give us information about a document. This assumes if a word appears more often in a document, it's more informative. However, this may not always be the case. For example, we've already removed stop words because they are not informative, despite the fact that they appear many times in a document. We also know the word \"flight\" is among the most frequent words, but it is not that informative, because it appears in many documents. Since we're looking at airline tweets, we shouldn't be surprised to see the word \"flight\"!\n",
    "\n",
    "To remedy this, we use a weighting scheme called **tf-idf (term frequency-inverse document frequency)**. The big idea behind tf-idf is to weight a word not just by its frequency within a document, but also by its frequency in one document relative to the remaining documents. So, when we construct the DTM, we will be assigning each term a **tf-idf score**. Specifically, term $t$ in document $d$ is assigned a tf-idf score as follows:\n",
    "\n",
    "<img src='../images/tf-idf_finalized.png' alt=\"TF-IDF\" width=\"1200\">\n",
    "\n",
    "In essence, the tf-idf score of a word in a document is the product of two components: **term frequency (tf)** and **inverse document frequency (idf)**. The idf acts as a scaling factor. If a word occurs in all documents, then idf equals 1. No scaling will happen. But idf is typically greater than 1, which is the weight we assign to the word to make the tf-idf score higher, so as to highlight that the word is informative. In practice, we add 1 to both the denominator and numerator (\"add-1 smooth\") to prevent any issues with zero occurrences.\n",
    "\n",
    "We can also create a tf-idf DTM using `sklearn`. We'll use a `TfidfVectorizer` this time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f5e32d8a-c42d-475f-aab4-21eca8b1aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa TfidfVectorizer de scikit-learn, usado para convertir texto en una matriz TF-IDF\n",
    "# TF-IDF pondera las palabras según su frecuencia en un documento y su rareza en todo el corpus\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d23916c1-5693-456c-b71d-6d9d78d1e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tfidf vectorizer\n",
    "vectorizer = TfidfVectorizer(lowercase=True, # Convierte todo el texto a minúsculas antes de vectorizar\n",
    "                             stop_words='english', # Elimina palabras vacías (stop words) en inglés\n",
    "                             min_df=2, # Ignora palabras que aparecen en menos de 2 documentos\n",
    "                             max_df=0.95, # Ignora palabras que aparecen en más del 95% de los documentos\n",
    "                             max_features=None) # No limitar el número de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7af5b342-ab18-4766-9561-e38e50cd1e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 119862 stored elements and shape (14640, 5090)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform \n",
    "tf_dtm = vectorizer.fit_transform(tweets['text_lemmatized'])# Ajusta el vectorizador TF-IDF a los tweets lematizados y transforma el texto en una matriz TF-IDF\n",
    "tf_dtm # Muestra la matriz TF-IDF resultante (matriz dispersa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55e509c8-5402-4be0-9143-0e448fff7066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>00am</th>\n",
       "      <th>00pm</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>05</th>\n",
       "      <th>0510</th>\n",
       "      <th>05am</th>\n",
       "      <th>05pm</th>\n",
       "      <th>...</th>\n",
       "      <th>yvonne</th>\n",
       "      <th>yvr</th>\n",
       "      <th>yyj</th>\n",
       "      <th>yyz</th>\n",
       "      <th>zero</th>\n",
       "      <th>zik2uoxgnw</th>\n",
       "      <th>zkatcher</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zurich</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5090 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00  000  00am  00pm   02   03   05  0510  05am  05pm  ...  yvonne  yvr  \\\n",
       "0  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "1  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "2  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "3  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "4  0.0  0.0   0.0   0.0  0.0  0.0  0.0   0.0   0.0   0.0  ...     0.0  0.0   \n",
       "\n",
       "   yyj  yyz  zero  zik2uoxgnw  zkatcher  zone  zoom  zurich  \n",
       "0  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "1  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "2  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "3  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "4  0.0  0.0   0.0         0.0       0.0   0.0   0.0     0.0  \n",
       "\n",
       "[5 rows x 5090 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tf-idf dataframe\n",
    "# Crear un DataFrame TF-IDF a partir de la matriz dispersa\n",
    "tfidf = pd.DataFrame(tf_dtm.todense(),  # Convertir la matriz TF-IDF dispersa a matriz densa\n",
    "                     columns=vectorizer.get_feature_names_out(), # Usar los tokens (palabras) como nombres de columnas\n",
    "                     index=tweets.index)  # Mantener los índices originales de los tweets\n",
    "tfidf.head() # Mostrar las primeras 5 filas del DataFrame TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba13ea-c429-4ff1-a9a2-abf27c4d0888",
   "metadata": {},
   "source": [
    "You may have noticed that the vocabulary size is the same as we saw in Challenge 2. This is because we used the same parameter setting when creating the vectorizer. But the values in the matrix are different—they are tf-idf scores instead of raw counts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58c360-5c55-4fa0-8c55-1f00e68baa9a",
   "metadata": {},
   "source": [
    "## Interpret TF-IDF Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdad233d-ebc1-420f-9b67-c227c48f3e60",
   "metadata": {},
   "source": [
    "Let's take a look the document where a term has the highest tf-idf values. We'll use the `.idxmax()` method to find the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "995b511a-d448-4cfb-a6a0-22a465efd8a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "00            10280\n",
       "000            3288\n",
       "00am          10222\n",
       "00pm          11271\n",
       "02             7905\n",
       "              ...  \n",
       "zik2uoxgnw    11888\n",
       "zkatcher       8389\n",
       "zone           3975\n",
       "zoom           4970\n",
       "zurich         2727\n",
       "Length: 5090, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the document\n",
    "tfidf.idxmax() # Obtener el índice del tweet donde cada palabra tiene su valor TF-IDF máximo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc0249-7c68-42ee-8290-ff41715e346b",
   "metadata": {},
   "source": [
    "For example, the term \"worst\" occurs most distinctively in the 918th tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09b222fb-ad8c-4767-a974-dd261370a06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(10265)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idxmax()['worst']# Obtener el índice del tweet donde la palabra 'worst' tiene el valor TF-IDF más alto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a48bc-dc93-481b-ba49-29876fc577fb",
   "metadata": {},
   "source": [
    "Recall that this is the tweet where the word \"worst\" appears six times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "079ee0e0-476f-4236-ba8a-615ba7a0efe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@united agent in las letting 20 customers know they can't help them rebook delayed flight to den #unfriendlyskies http://t.co/quzvmk2rtr\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text_processed'].iloc[918]# Obtiene el tweet preprocesado en la posición 918\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd06bbc-e2fc-49e4-9354-efdaca5cfbd3",
   "metadata": {},
   "source": [
    "How about \"cancel\"? Let's take a look at another example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f809df1a-1178-4272-a415-42edb20173b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(7840)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.idxmax()['cancel'] # Obtener el índice del tweet donde la palabra 'cancel' tiene el valor TF-IDF más alto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8093b6a7-54ca-468a-9376-b3c0be0b6f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@southwestair you need to get your act together. you new this morning at 830 our plane was malfunctioning. yet i've been delayed 3 times ..\""
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['text_processed'].iloc[5945]# Obtiene el tweet preprocesado en la posición 5945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dcecd-dc8c-43a9-952d-5bc84a307b07",
   "metadata": {},
   "source": [
    "## 🥊 Challenge 3: Words with Highest Mean TF-IDF scores\n",
    "\n",
    "We have obtained tf-idf values for each term in each document. But what do these values tell us about the sentiments of tweets? Are there any words that are  particularly informative for positive/negative tweets? \n",
    "\n",
    "To explore this, let's gather the indices of all positive/negative tweets and calculate the mean tf-idf scores of words appear in each category. \n",
    "\n",
    "We've provided the following starter code to guide you:\n",
    "- Subset the `tweets` dataframe according to the `airline_sentiment` label and retrieve the index of each subset (`.index`). Assign the index to `positive_index` or `negative_index`.\n",
    "- For each subset:\n",
    "    - Retrieve the td-idf representation \n",
    "    - Take the mean tf-idf values across the subset using `.mean()`\n",
    "    - Sort the mean values in the descending order using `.sort_values()`\n",
    "    - Get the top 10 terms using `.head()`\n",
    "\n",
    "Next, run `pos.plot` and `neg.plot` to plot the words with the highest mean tf-idf scores for each subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2bfbf838-9ff6-48b8-ad5d-5e75304fe060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the boolean masks \n",
    "positive_index = tweets[tweets['airline_sentiment'] == 'positive'].index # Índices de tweets con sentimiento positivo\n",
    "negative_index = tweets[tweets['airline_sentiment'] == 'negative'].index # Índices de tweets con sentimiento negativo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c67ea1f-de9e-49a9-94f2-a3351446e364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the following two lines\n",
    "# Obtenerlas palabras más frecuentes en tweets positivos\n",
    "pos = tfidf.loc[positive_index]       # Selecciona los tweets con sentimiento positivo usando sus índices\n",
    "pos = pos.mean()                      # Calcula el valor promedio TF-IDF de cada palabra en esos tweets\n",
    "pos = pos.sort_values(ascending=False) # Ordena de mayor a menor importancia\n",
    "pos = pos.head(10)   \n",
    "# Obtener las palabras más frecuentes en tweets negativos\n",
    "neg = tfidf.loc[negative_index]       # Selecciona los tweets con sentimiento negativo usando sus índices\n",
    "neg = neg.mean()                      # Calcula el valor promedio TF-IDF de cada palabra en esos tweets\n",
    "neg = neg.sort_values(ascending=False) # Ordena de mayor a menor importancia\n",
    "neg = neg.head(10)                     # Selecciona las 10 palabras más representativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e29043-8c78-4e41-81d2-b4552030b457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las 10 palabras más representativas de tweets positivos usando un gráfico de barras horizontal\n",
    "pos.plot(kind='barh', # Tipo de gráfico: barras horizontales\n",
    "         xlim=(0, 0.18), # Limitar el eje X entre 0 y 0.18\n",
    "         color='cornflowerblue',  # Color de las barras\n",
    "         title='Top 10 terms with the highest mean tf-idf values for positive tweets');# Título del gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b25940-2372-4755-818e-f75e4d23daf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar las 10 palabras más representativas de tweets negativos usando un gráfico de barras horizontal\n",
    "neg.plot(kind='barh', # Tipo de gráfico: barras horizontales\n",
    "         xlim=(0, 0.18), # Limitar el eje X entre 0 y 0.18\n",
    "         color='darksalmon', # Color de las barras\n",
    "         title='Top 10 terms with the highest mean tf-idf values for negative tweets');# Título del gráfico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bca876-9649-46f3-bd4f-f9f68fea649a",
   "metadata": {},
   "source": [
    "🔔 **Question**: How would you interpret these results? Share your thoughts in the chat!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da410cb3-a452-441b-a94d-8f751d59d7a6",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "\n",
    "## 🎬 **Demo**: Sentiment Classification Using the TF-IDF Representation\n",
    "\n",
    "Now that we have a tf-idf representation of the text, we are ready to do sentiment analysis!\n",
    "\n",
    "In this demo, we will use a logistic regression model to perform the classification task. Here we briefly step through how logistic regression works as one of the supervised Machine Learning methods, but feel free to explore our workshop on [Python Machine Learning Fundamentals](https://github.com/dlab-berkeley/Python-Machine-Learning) if you want to learn more about it.\n",
    "\n",
    "Logistic regression is a linear model, with which we use to predict the label of a tweet, based on a set of features ($x_1, x_2, x_3, ..., x_i$), as shown below:\n",
    "\n",
    "$$\n",
    "L = \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_T x_T\n",
    "$$\n",
    "\n",
    "The list of features we'll pass to the model is the vocabulary of the DTM. We also feed the model with a portion of the data, known as the training set, along with other model specification, to learn the coeffient ($\\beta_1, \\beta_2, \\beta_3, ..., \\beta_i$) of each feature. The coefficients tell us whether a feature contributes positively or negatively to the predicted value. The predicted value corresponds to adding all features (multiplied by their coefficients) up, and the predicted value gets passed to a [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) to be converted into the probability space, which tells us whether the predicted label is positive (when $p>0.5$) or negative (when $p<0.5$). \n",
    "\n",
    "The remaining portion of the data, known as the test set, is used to test whether the learned coefficients could be generalized to unseen data. \n",
    "\n",
    "Now that we already have the tf-idf dataframe, the feature set is ready. Let's dive into model specification!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33413d63-87eb-489f-b374-3cfeaa51cf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV # Importar el modelo de regresión logística con validación cruzada\n",
    "from sklearn.model_selection import train_test_split # Importar función para dividir los datos en conjuntos de entrenamiento y prueba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee87ff74-3fbb-472a-b795-6f4d18fab215",
   "metadata": {},
   "source": [
    "We'll use the `train_test_split` function from `sklearn` to separate our data into two sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cec8b9-14d9-4897-9c02-cc89fcf7b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "# Definir las variables predictoras (features) y la variable objetivo (target)\n",
    "X = tfidf # Variables predictoras: matriz TF-IDF de los tweets\n",
    "y = tweets['airline_sentiment'] # Variable objetivo: sentimiento de cada tweet\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, # Datos y etiquetas\n",
    "    test_size=0.15 # Proporción del 15% para el conjunto de prueba\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066771d8-2f31-4646-9a1b-6d2b1b9b208c",
   "metadata": {},
   "source": [
    "The `fit_logistic_regression` function is written below to streamline the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d46de0b2-af00-4a1d-b4cd-31b96ce545d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir una función para entrenar un modelo de regresión logística\n",
    "def fit_logistic_regression(X, y):\n",
    "    '''Fits a logistic regression model to provided data.'''\n",
    "    # Crear y entrenar el modelo de regresión logística con validación cruzada\n",
    "    model = LogisticRegressionCV(Cs=10,# Número de valores de regularización C a probar\n",
    "                                 penalty='l1', # Tipo de penalización L1 (Lasso) para selección de variables\n",
    "                                 cv=5,# Número de pliegues para validación cruzada\n",
    "                                 solver='liblinear', # Algoritmo de optimización (compatible con L1)\n",
    "                                 class_weight='balanced',# Ajusta pesos para manejar clases desbalanceadas\n",
    "                                 random_state=42,# Fijar semilla para reproducibilidad\n",
    "                                 refit=True # Reentrena el modelo final usando todos los datos\n",
    "                                 ).fit(X, y)# Ajusta el modelo a los datos proporcionados\n",
    "    return model # Devuelve el modelo entrenado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124aa7ea-1bc1-43e2-beeb-0ba2da9b2df9",
   "metadata": {},
   "source": [
    "We'll fit the model and compute the training and test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773963bd-6603-4fad-884b-09ce60afab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the logistic regression model\n",
    "model = fit_logistic_regression(X_train, y_train)# Ajusta el modelo de regresión logística usando los datos de entrenamiento\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10d06c1-d884-45d4-a03d-dd5d40bf70aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the training and test accuracy\n",
    "print(f\"Training accuracy: {model.score(X_train, y_train)}\")# Muestra la precisión (accuracy) del modelo en los datos de entrenamiento\n",
    "print(f\"Test accuracy: {model.score(X_test, y_test)}\")# Muestra la precisión (accuracy) del modelo en los datos de prueba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e186c5-1719-4deb-bdb4-614a9980f058",
   "metadata": {},
   "source": [
    "The model achieved ~94% accuracy on the training set and ~89% on the test set—that's pretty good! The model generalizes reasonably well to the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310dac39-4753-4ae8-8dfa-e65e5824cccb",
   "metadata": {},
   "source": [
    "Next, let's also take a look at the fitted coefficients to see if what we see makes sense. \n",
    "\n",
    "We can access them using `coef_`, and we can match each coefficient to the tokens from the vectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcb6ef1-13b3-437e-813c-7118911847a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get coefs of all features\n",
    "coefs = model.coef_.ravel()# Obtener los coeficientes (pesos) que el modelo de regresión logística aprendió\n",
    "\n",
    "# Get all tokens\n",
    "tokens = vectorizer.get_feature_names_out()# Obtener la lista de tokens (palabras) del vocabulario generado por el vectorizador TF-IDF\n",
    "\n",
    "# Create a token-coef dataframe\n",
    "importance = pd.DataFrame()# Crear un DataFrame vacío para organizar la relación entre tokens y coeficientes\n",
    "importance['token'] = tokens # Agregar la columna \"token\" con todas las palabras del vocabulario\n",
    "importance['coefs'] = coefs # Agregar la columna \"coefs\" con los valores de los coeficientes aprendidos para cada palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e63814e-9c0d-4f7a-a5e0-72cca2758d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 tokens with lowest coefs\n",
    "# Ordena el DataFrame 'importance' por la columna 'coefs' en orden ascendente\n",
    "# De esta forma, los coeficientes más negativos (palabras asociadas a sentimientos negativos) quedan arriba\n",
    "neg_coef = importance.sort_values('coefs').head(10)\n",
    "neg_coef # Muestra las primeras 10 filas de ese nuevo DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d596bf7-753c-40cd-ac52-4a37163650ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 tokens with highest coefs\n",
    "# Ordena el DataFrame 'importance' por la columna 'coefs' en orden ascendente\n",
    "# y selecciona las últimas 10 filas, que corresponden a los coeficientes más altos (positivos)\n",
    "pos_coef = importance.sort_values('coefs').tail(10)\n",
    "pos_coef "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b7893-caa0-4281-98f0-92c9e7b31953",
   "metadata": {},
   "source": [
    "Let's plot the top 10 tokens with the highest/lowest coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b1223b-e5c1-4992-bb7e-0a99651c3729",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance.head()\n",
    "pos_coef = importance.sort_values(by='coefs', ascending=True).tail(10)\n",
    "# Plot the top 10 tokens that have the highest coefs\n",
    "# Ordena el DataFrame 'pos_coef' de mayor a menor según la columna 'coefs'\n",
    "pos_coef.sort_values('coefs', ascending=False) \\\n",
    "        .plot(kind='barh', # Genera un gráfico de barras horizontal (barh) con los datos ordenados\n",
    "              xlim=(0, 18), # Establece el rango del eje X entre 0 y 18\n",
    "              x='token', # Define que el eje Y (categorías) muestre el valor de la columna 'token'\n",
    "              color='cornflowerblue', # Asigna el color 'cornflowerblue' a las barras\n",
    "              title='Top 10 tokens with highest coeffient values'); # Coloca un título al gráfico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e00c6-8a9f-484f-aea2-853fd5512083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the top 10 tokens that have the lowest coefs\n",
    "neg_coef.plot( \n",
    "                kind='barh',       # Tipo de gráfico: barras horizontales\n",
    "                xlim=(0, -18),     # Limite del eje x (aunque negativo aquí probablemente sea un error)\n",
    "                x='token',         # Define que el eje Y muestre los nombres de los tokens\n",
    "                color='darksalmon',# Color de las barras\n",
    "                title='Top 10 tokens with lowest coeffient values' # Título del gráfico\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed48ea-fd35-4585-9b98-90456aaee447",
   "metadata": {},
   "source": [
    "Words like \"ruin,\" \"rude,\" and \"hour\" are strong indicators of negative sentiment, while \"thank,\" \"awesome,\" and \"wonderful\" are associated with positive sentiment. \n",
    "\n",
    "We will wrap up Part 2 with these plots. These coefficient terms and the words with the highest TF-IDF values provide different perspectives on the sentiment of tweets. If you'd like, take some time to compare the two sets of plots and see which one provides a better account of the sentiments conveyed in tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4430fbd-108f-4a02-ab64-ef36c5949e56",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ❗ Key Points\n",
    "\n",
    "* A Bag-of-Words representation is a simple method to transform our text data to numbers. It focuses on word frequency but not word order. \n",
    "* A TF-IDF representation is a step further; it also considers if a certain word distinctively appears in one document or occurs uniformally across all documents. \n",
    "* With a numerical representation, we can perform a range of text classification task, such as sentiment analysis. \n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
