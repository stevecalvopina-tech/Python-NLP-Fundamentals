{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3e7ea21-6437-48e8-a9e4-3bdc05f709c9",
   "metadata": {},
   "source": [
    "# Python Text Analysis: Preprocessing\n",
    "\n",
    "* * * \n",
    "\n",
    "<div class=\"alert alert-success\">  \n",
    "    \n",
    "### Learning Objectives \n",
    "    \n",
    "* Learn common steps for preprocessing text data, as well as specific operations for preprocessing Twitter data.\n",
    "* Know commonly used NLP packages and what they are capable of.\n",
    "* Understand tokenizers, and how they have changed since the advent of Large Language Models.\n",
    "</div>\n",
    "\n",
    "### Icons Used in This Notebook\n",
    "üîî **Question**: A quick question to help you understand what's going on.<br>\n",
    "ü•ä **Challenge**: Interactive excersise. We'll work through these in the workshop!<br>\n",
    "‚ö†Ô∏è **Warning:** Heads-up about tricky stuff or common mistakes.<br>\n",
    "üé¨ **Demo**: Showing off something more advanced ‚Äì so you know what Python can be used for!<br> \n",
    "\n",
    "### Sections\n",
    "1. [Preprocessing](#section1)\n",
    "2. [Tokenization](#section2)\n",
    "\n",
    "In this three-part workshop series, we'll learn the building blocks for performing text analysis in Python. These techniques lie in the domain of Natural Language Processing (NLP). NLP is a field that deals with identifying and extracting patterns of language, primarily in written texts. Throughout the workshop series, we'll interact with various packages for performing text analysis: starting from simple string methods to specific NLP packages, such as `nltk`, `spaCy`, and more recent ones on Large Language Models (`BERT`).\n",
    "\n",
    "Now, let's have these packages properly installed before diving into the materials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d442e4c7-e926-493d-a64e-516616ad915a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting NLTK\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from NLTK)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting joblib (from NLTK)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting regex>=2021.8.3 (from NLTK)\n",
      "  Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from NLTK)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from click->NLTK) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 1.0/1.5 MB 4.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 3.6 MB/s  0:00:00\n",
      "Downloading regex-2025.7.34-cp313-cp313-win_amd64.whl (275 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, NLTK\n",
      "\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   ---------------------------------------- 0/5 [tqdm]\n",
      "   -------- ------------------------------- 1/5 [regex]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ---------------- ----------------------- 2/5 [joblib]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   ------------------------ --------------- 3/5 [click]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   -------------------------------- ------- 4/5 [NLTK]\n",
      "   ---------------------------------------- 5/5 [NLTK]\n",
      "\n",
      "Successfully installed NLTK-3.9.1 click-8.2.1 joblib-1.5.1 regex-2025.7.34 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.19.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from transformers) (25.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting typing-extensions>=3.7.4.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset_normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.3-cp313-cp313-win_amd64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/11.3 MB 4.6 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.2/11.3 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.1/11.3 MB 12.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 7.3/11.3 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.4/11.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.3 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.3/11.3 MB 8.6 MB/s  0:00:01\n",
      "Downloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "   ---------------------------------------- 0.0/561.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 561.5/561.5 kB 9.4 MB/s  0:00:00\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-win_amd64.whl (2.5 MB)\n",
      "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.5/2.5 MB 18.1 MB/s  0:00:00\n",
      "Downloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading PyYAML-6.0.2-cp313-cp313-win_amd64.whl (156 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-win_amd64.whl (320 kB)\n",
      "Downloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
      "Downloading filelock-3.19.1-py3-none-any.whl (15 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.3-cp313-cp313-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, safetensors, pyyaml, idna, fsspec, filelock, charset_normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ----------------------------------------  0/13 [urllib3]\n",
      "   ----------------------------------------  0/13 [urllib3]\n",
      "   ----------------------------------------  0/13 [urllib3]\n",
      "   ----------------------------------------  0/13 [urllib3]\n",
      "   ------ ---------------------------------  2/13 [safetensors]\n",
      "   ------ ---------------------------------  2/13 [safetensors]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   ------------ ---------------------------  4/13 [idna]\n",
      "   ------------ ---------------------------  4/13 [idna]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [fsspec]\n",
      "   ------------------ ---------------------  6/13 [filelock]\n",
      "   ------------------ ---------------------  6/13 [filelock]\n",
      "   --------------------- ------------------  7/13 [charset_normalizer]\n",
      "   --------------------- ------------------  7/13 [charset_normalizer]\n",
      "   --------------------- ------------------  7/13 [charset_normalizer]\n",
      "   ------------------------ ---------------  8/13 [certifi]\n",
      "   --------------------------- ------------  9/13 [requests]\n",
      "   --------------------------- ------------  9/13 [requests]\n",
      "   --------------------------- ------------  9/13 [requests]\n",
      "   --------------------------- ------------  9/13 [requests]\n",
      "   --------------------------- ------------  9/13 [requests]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   ------------------------------ --------- 10/13 [huggingface-hub]\n",
      "   --------------------------------- ------ 11/13 [tokenizers]\n",
      "   --------------------------------- ------ 11/13 [tokenizers]\n",
      "   --------------------------------- ------ 11/13 [tokenizers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ------------------------------------ --- 12/13 [transformers]\n",
      "   ---------------------------------------- 13/13 [transformers]\n",
      "\n",
      "Successfully installed certifi-2025.8.3 charset_normalizer-3.4.3 filelock-3.19.1 fsspec-2025.7.0 huggingface-hub-0.34.4 idna-3.10 pyyaml-6.0.2 requests-2.32.5 safetensors-0.6.2 tokenizers-0.21.4 transformers-4.55.4 typing-extensions-4.14.1 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spaCy\n",
      "  Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spaCy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spaCy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spaCy)\n",
      "  Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spaCy)\n",
      "  Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spaCy)\n",
      "  Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spaCy)\n",
      "  Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spaCy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spaCy)\n",
      "  Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spaCy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spaCy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spaCy)\n",
      "  Downloading typer-0.16.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.3.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (2.32.5)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spaCy)\n",
      "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
      "Collecting jinja2 (from spaCy)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting setuptools (from spaCy)\n",
      "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from spaCy) (25.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spaCy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spaCy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy) (4.14.1)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spaCy)\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2025.8.3)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spaCy)\n",
      "  Downloading blis-1.3.0-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spaCy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spaCy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spaCy) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spaCy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spaCy)\n",
      "  Downloading rich-14.1.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spaCy)\n",
      "  Downloading cloudpathlib-0.21.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spaCy)\n",
      "  Downloading smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spaCy)\n",
      "  Downloading wrapt-1.17.3-cp313-cp313-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spaCy)\n",
      "  Downloading marisa_trie-1.3.0-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\geoco\\onedrive\\escritorio\\uide\\practicas\\analisis_datos\\venv_da\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spaCy)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spaCy)\n",
      "  Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 1.8/13.9 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 6.6/13.9 MB 14.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 15.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 15.5 MB/s  0:00:01\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl (115 kB)\n",
      "Downloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 18.7 MB/s  0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl (630 kB)\n",
      "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 630.6/630.6 kB 12.0 MB/s  0:00:00\n",
      "Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 19.9 MB/s  0:00:00\n",
      "Downloading blis-1.3.0-cp313-cp313-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 4.7/6.3 MB 23.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 21.5 MB/s  0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading typer-0.16.1-py3-none-any.whl (46 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading cloudpathlib-0.21.1-py3-none-any.whl (52 kB)\n",
      "Downloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 4.5/5.4 MB 22.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 20.5 MB/s  0:00:00\n",
      "Downloading marisa_trie-1.3.0-cp313-cp313-win_amd64.whl (139 kB)\n",
      "Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.2/1.2 MB 19.7 MB/s  0:00:00\n",
      "Downloading wrapt-1.17.3-cp313-cp313-win_amd64.whl (38 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, typing-inspection, spacy-loggers, spacy-legacy, shellingham, setuptools, pydantic-core, murmurhash, mdurl, MarkupSafe, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, smart-open, pydantic, preshed, markdown-it-py, language-data, jinja2, rich, langcodes, confection, typer, thinc, weasel, spaCy\n",
      "\n",
      "   - --------------------------------------  1/31 [wrapt]\n",
      "   -- -------------------------------------  2/31 [wasabi]\n",
      "   -- -------------------------------------  2/31 [wasabi]\n",
      "   -- -------------------------------------  2/31 [wasabi]\n",
      "   ----- ----------------------------------  4/31 [spacy-loggers]\n",
      "   ----- ----------------------------------  4/31 [spacy-loggers]\n",
      "   ----- ----------------------------------  4/31 [spacy-loggers]\n",
      "   ------ ---------------------------------  5/31 [spacy-legacy]\n",
      "   ------ ---------------------------------  5/31 [spacy-legacy]\n",
      "   ------ ---------------------------------  5/31 [spacy-legacy]\n",
      "   ------ ---------------------------------  5/31 [spacy-legacy]\n",
      "   ------ ---------------------------------  5/31 [spacy-legacy]\n",
      "   ------- --------------------------------  6/31 [shellingham]\n",
      "   ------- --------------------------------  6/31 [shellingham]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   --------- ------------------------------  7/31 [setuptools]\n",
      "   ---------- -----------------------------  8/31 [pydantic-core]\n",
      "   ---------- -----------------------------  8/31 [pydantic-core]\n",
      "   ------------ --------------------------- 10/31 [mdurl]\n",
      "   ------------ --------------------------- 10/31 [mdurl]\n",
      "   -------------- ------------------------- 11/31 [MarkupSafe]\n",
      "   ---------------- ----------------------- 13/31 [cloudpathlib]\n",
      "   ---------------- ----------------------- 13/31 [cloudpathlib]\n",
      "   ---------------- ----------------------- 13/31 [cloudpathlib]\n",
      "   ---------------- ----------------------- 13/31 [cloudpathlib]\n",
      "   ---------------- ----------------------- 13/31 [cloudpathlib]\n",
      "   ------------------ --------------------- 14/31 [catalogue]\n",
      "   ------------------- -------------------- 15/31 [blis]\n",
      "   ------------------- -------------------- 15/31 [blis]\n",
      "   -------------------- ------------------- 16/31 [annotated-types]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   --------------------- ------------------ 17/31 [srsly]\n",
      "   ----------------------- ---------------- 18/31 [smart-open]\n",
      "   ----------------------- ---------------- 18/31 [smart-open]\n",
      "   ----------------------- ---------------- 18/31 [smart-open]\n",
      "   ----------------------- ---------------- 18/31 [smart-open]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------ --------------- 19/31 [pydantic]\n",
      "   ------------------------- -------------- 20/31 [preshed]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   --------------------------- ------------ 21/31 [markdown-it-py]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ---------------------------- ----------- 22/31 [language-data]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ----------------------------- ---------- 23/31 [jinja2]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   ------------------------------ --------- 24/31 [rich]\n",
      "   -------------------------------- ------- 25/31 [langcodes]\n",
      "   -------------------------------- ------- 25/31 [langcodes]\n",
      "   -------------------------------- ------- 25/31 [langcodes]\n",
      "   -------------------------------- ------- 25/31 [langcodes]\n",
      "   --------------------------------- ------ 26/31 [confection]\n",
      "   --------------------------------- ------ 26/31 [confection]\n",
      "   --------------------------------- ------ 26/31 [confection]\n",
      "   ---------------------------------- ----- 27/31 [typer]\n",
      "   ---------------------------------- ----- 27/31 [typer]\n",
      "   ---------------------------------- ----- 27/31 [typer]\n",
      "   ---------------------------------- ----- 27/31 [typer]\n",
      "   ---------------------------------- ----- 27/31 [typer]\n",
      "   ---------------------------------- ----- 27/31 [typer]\n",
      "   ---------------------------------- ----- 27/31 [typer]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------ --- 28/31 [thinc]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   ------------------------------------- -- 29/31 [weasel]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   -------------------------------------- - 30/31 [spaCy]\n",
      "   ---------------------------------------- 31/31 [spaCy]\n",
      "\n",
      "Successfully installed MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.21.1 confection-0.1.5 cymem-2.0.11 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.0 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 preshed-3.0.10 pydantic-2.11.7 pydantic-core-2.33.2 rich-14.1.0 setuptools-80.9.0 shellingham-1.5.4 smart-open-7.3.0.post1 spaCy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.16.1 typing-inspection-0.4.1 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---- ----------------------------------- 1.6/12.8 MB 13.2 MB/s eta 0:00:01\n",
      "     -------------- ------------------------- 4.7/12.8 MB 14.0 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 14.5 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 10.0/12.8 MB 14.7 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 10.2/12.8 MB 12.9 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 8.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 7.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.1/12.8 MB 7.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 6.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.6 MB/s  0:00:02\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Uncomment the following lines to install packages/model\n",
    "%pip install NLTK #Instala la librer√≠a NLTK para procesamiento de lenguaje natural cl√°sico.\n",
    "%pip install transformers #Instala transformers de Hugging Face para usar modelos de NLP preentrenados como BERT o GPT.\n",
    "%pip install spaCy #Instala spaCy, otra librer√≠a de NLP r√°pida y orientada a producci√≥n.\n",
    "!python -m spacy download en_core_web_sm #Descarga el modelo peque√±o de ingl√©s en_core_web_sm para spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5b8f8e-4e69-426e-a202-ec48b325e89a",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "In Part 1 of this workshop, we'll address the first step of text analysis. Our goal is to convert the raw, messy text data into a consistent format. This process is often called **preprocessing**, **text cleaning**, or **text normalization**.\n",
    "\n",
    "You'll notice that at the end of preprocessing, our data is still in a format that we can read and understand. In Parts 2 and 3, we will begin our foray into converting the text data into a numerical representation‚Äîa format that can be more readily handled by computers. \n",
    "\n",
    "üîî **Question**: Let's pause for a minute to reflect on **your** previous experiences working on text data. \n",
    "- What is the format of the text data you have interacted with (plain text, CSV, or XML)?\n",
    "- Where does it come from (structured corpus, scraped from the web, survey data)?\n",
    "- Is it messy (i.e., is the data formatted consistently)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35911a-3b3f-4a48-a7d1-9882aab04851",
   "metadata": {},
   "source": [
    "## Common Processes\n",
    "\n",
    "Preprocessing is not something we can accomplish with a single line of code. We often start by familiarizing ourselves with the data, and along the way, we gain a clearer understanding of the granularity of preprocessing we want to apply.\n",
    "\n",
    "Typically, we begin by applying a set of commonly used processes to clean the data. These operations don't substantially alter the form or meaning of the data; they serve as a standardized procedure to reshape the data into a consistent format.\n",
    "\n",
    "The following processes, for examples, are commonly applied to preprocess English texts of various genres. These operations can be done using built-in Python functions, such as `string` methods, and Regular Expressions. \n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "- Remove stop words\n",
    "\n",
    "After the initial processing, we may choose to perform task-specific processes, the specifics of which often depend on the downstream task we want to perform and the nature of the text data (i.e., its stylistic and linguistic features).  \n",
    "\n",
    "Before we jump into these operations, let's take a look at our data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5d7350-9a1e-4db9-b828-a87fe1676d8d",
   "metadata": {},
   "source": [
    "### Import the Text Data\n",
    "\n",
    "The text data we'll be working with is a CSV file. It contains tweets about U.S. airlines, scrapped from Feb 2015. \n",
    "\n",
    "Let's read the file `airline_tweets.csv` into dataframe with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1ff64b-53ad-4eca-b846-3fda20085c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd #Importa la librer√≠a pandas y le asigna el alias pd (forma est√°ndar para trabajar con pandas en Python).\n",
    "\n",
    "# File path to data\n",
    "csv_path = '../data/airline_tweets.csv' #Define la ruta del archivo CSV que contiene los datos, en este caso airline_tweets.csv que est√° en la carpeta ../data/.\n",
    "\n",
    "# Specify the separator\n",
    "tweets = pd.read_csv(csv_path, sep=',') # Lee el archivo CSV usando pandas y lo guarda en el DataFrame tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397ac6a-c2ba-4cce-8700-b36b38026c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the first five rows\n",
    "tweets.head() #Muestra por defecto las primeras 5 filas del DataFrame tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b339f-45cf-465d-931c-05f9096fd510",
   "metadata": {},
   "source": [
    "The dataframe has one row per tweet. The text of tweet is shown in the `text` column.\n",
    "- `text` (`str`): the text of the tweet.\n",
    "\n",
    "Other metadata we are interested in include: \n",
    "- `airline_sentiment` (`str`): the sentiment of the tweet, labeled as \"neutral,\" \"positive,\" or \"negative.\"\n",
    "- `airline` (`str`): the airline that is tweeted about.\n",
    "- `retweet count` (`int`): how many times the tweet was retweeted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c695b-4bd1-4151-9cb9-ef5253eb16df",
   "metadata": {},
   "source": [
    "Let's take a look at some of the tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b690daab-7be5-4b8f-8af0-a91fdec4ec4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica What @dhepburn said.\n",
      "@VirginAmerica plus you've added commercials to the experience... tacky.\n",
      "@VirginAmerica I didn't today... Must mean I need to take another trip!\n"
     ]
    }
   ],
   "source": [
    "print(tweets['text'].iloc[0]) # Imprime el texto del primer tweet (fila 0 de la columna 'text')\n",
    "print(tweets['text'].iloc[1]) # Imprime el texto del segundo tweet (fila 1 de la columna 'text')\n",
    "print(tweets['text'].iloc[2]) # Imprime el texto del tercer tweet (fila 2 de la columna 'text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc05fa-ad30-4402-ab56-086bcb09a166",
   "metadata": {},
   "source": [
    "üîî **Question**: What have you noticed? What are the stylistic features of tweets?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3460393-00a6-461c-b02a-9e98f9b5d1af",
   "metadata": {},
   "source": [
    "### Lowercasing\n",
    "\n",
    "While we acknowledge that a word's casing is informative, we often don't work in contexts where we can properly utilize this information.\n",
    "\n",
    "More often, the subsequent analysis we perform is **case-insensitive**. For instance, in frequency analysis, we want to account for various forms of the same word. Lowercasing the text data aids in this process and simplifies our analysis.\n",
    "\n",
    "We can easily achieve lowercasing with the string method [`.lower()`](https://docs.python.org/3/library/stdtypes.html#str.lower); see [documentation](https://docs.python.org/3/library/stdtypes.html#string-methods) for more useful functions.\n",
    "\n",
    "Let's apply it to the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a95d90-3ef1-4bff-9cfe-d447ed99f252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica I was scheduled for SFO 2 DAL flight 714 today. Changed to 24th due weather. Looks like flight still on?\n"
     ]
    }
   ],
   "source": [
    "# Print the first example\n",
    "first_example = tweets['text'][108] # Selecciona el texto del tweet que est√° en la posici√≥n 108 de la columna 'text'\n",
    "print(first_example) # Imprime ese tweet en pantalla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c66d91c0-6eed-4591-95fc-cd2eae2e0d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "==================================================\n",
      "@virginamerica i was scheduled for sfo 2 dal flight 714 today. changed to 24th due weather. looks like flight still on?\n",
      "==================================================\n",
      "@VIRGINAMERICA I WAS SCHEDULED FOR SFO 2 DAL FLIGHT 714 TODAY. CHANGED TO 24TH DUE WEATHER. LOOKS LIKE FLIGHT STILL ON?\n"
     ]
    }
   ],
   "source": [
    "# Check if all characters are in lowercase\n",
    "print(first_example.islower())# Verifica si todos los caracteres de 'first_example' est√°n en min√∫sculas\n",
    "print(f\"{'=' * 50}\")# Imprime una l√≠nea separadora de 50 signos \"=\"\n",
    "\n",
    "\n",
    "# Convert it to lowercase\n",
    "print(first_example.lower())# Convierte el texto a min√∫sculas y lo imprime\n",
    "print(f\"{'=' * 50}\")# Imprime nuevamente una l√≠nea separadora de 50 signos \"=\"\n",
    "\n",
    "# Convert it to uppercase\n",
    "print(first_example.upper())# Convierte el texto a may√∫sculas y lo imprime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf0d8c8-bd6c-47ef-b305-09ac61d07d4d",
   "metadata": {},
   "source": [
    "### Remove Extra Whitespace Characters\n",
    "\n",
    "Sometimes we might come across texts with extraneous whitespace, such as spaces, tabs, and newline characters, which is particularly common when the text is scrapped from web pages. Before we dive into the details, let's briefly introduce Regular Expressions (regex) and the `re` package. \n",
    "\n",
    "Regular expressions are a powerful way of searching for specific string patterns in large corpora. They have an infamously steep learning curve, but they can be very efficient when we get a handle on them. Many NLP packages heavily rely on regex under the hood. Regex testers, such as [regex101](https://regex101.com), are useful tools in both understanding and creating regex expressions.\n",
    "\n",
    "Our goal in this workshop is not to provide a deep (or even shallow) dive into regex; instead, we want to expose you to them so that you are better prepared to do deep dives in the future!\n",
    "\n",
    "The following example is a poem by William Wordsworth. Like many poems, the text may contain extra line breaks (i.e., newline characters, `\\n`) that we want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1bd73f1-a30f-4269-a05e-47cfff7b496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File path to the poem\n",
    "text_path = '../data/poem_wordsworth.txt'# Ruta del archivo de texto (el poema de Wordsworth)\n",
    "\n",
    "# Read the poem in\n",
    "with open(text_path, 'r') as file:# Abre el archivo en modo lectura ('r') y lo carga en la variable 'text'\n",
    "    text = file.read() # Lee todo el contenido del archivo y lo guarda en 'text'\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a693dd9-9706-40b3-863f-f568020245f7",
   "metadata": {},
   "source": [
    "As you can see, the poem is formatted as a continuous string of text with line breaks placed at the end of each line, making it difficult to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e78a75a-8e15-4bcb-a416-783aa7f60ef3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I wandered lonely as a cloud\\n\\n\\nI wandered lonely as a cloud\\nThat floats on high o'er vales and hills,\\nWhen all at once I saw a crowd,\\nA host, of golden daffodils;\\nBeside the lake, beneath the trees,\\nFluttering and dancing in the breeze.\\n\\nContinuous as the stars that shine\\nAnd twinkle on the milky way,\\nThey stretched in never-ending line\\nAlong the margin of a bay:\\nTen thousand saw I at a glance,\\nTossing their heads in sprightly dance.\\n\\nThe waves beside them danced; but they\\nOut-did the sparkling waves in glee:\\nA poet could not but be gay,\\nIn such a jocund company:\\nI gazed√¢‚Ç¨‚Äùand gazed√¢‚Ç¨‚Äùbut little thought\\nWhat wealth the show to me had brought:\\n\\nFor oft, when on my couch I lie\\nIn vacant or in pensive mood,\\nThey flash upon that inward eye\\nWhich is the bliss of solitude;\\nAnd then my heart with pleasure fills,\\nAnd dances with the daffodils.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text #La variable text contiene todo el contenido del archivo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cce993-c315-4aaa-87fe-149de8607f65",
   "metadata": {},
   "source": [
    "One handy function we can use to display the poem properly is `.splitlines()`. As the name suggests, it splits a long text sequence into a list of lines whenever there is a newline character.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddeade7a-065d-49e6-bdd3-87a8ea8f6e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I wandered lonely as a cloud',\n",
       " '',\n",
       " '',\n",
       " 'I wandered lonely as a cloud',\n",
       " \"That floats on high o'er vales and hills,\",\n",
       " 'When all at once I saw a crowd,',\n",
       " 'A host, of golden daffodils;',\n",
       " 'Beside the lake, beneath the trees,',\n",
       " 'Fluttering and dancing in the breeze.',\n",
       " '',\n",
       " 'Continuous as the stars that shine',\n",
       " 'And twinkle on the milky way,',\n",
       " 'They stretched in never-ending line',\n",
       " 'Along the margin of a bay:',\n",
       " 'Ten thousand saw I at a glance,',\n",
       " 'Tossing their heads in sprightly dance.',\n",
       " '',\n",
       " 'The waves beside them danced; but they',\n",
       " 'Out-did the sparkling waves in glee:',\n",
       " 'A poet could not but be gay,',\n",
       " 'In such a jocund company:',\n",
       " 'I gazed√¢‚Ç¨‚Äùand gazed√¢‚Ç¨‚Äùbut little thought',\n",
       " 'What wealth the show to me had brought:',\n",
       " '',\n",
       " 'For oft, when on my couch I lie',\n",
       " 'In vacant or in pensive mood,',\n",
       " 'They flash upon that inward eye',\n",
       " 'Which is the bliss of solitude;',\n",
       " 'And then my heart with pleasure fills,',\n",
       " 'And dances with the daffodils.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the single string into a list of lines\n",
    "text.splitlines()# Divide el texto en una lista de l√≠neas usando saltos de l√≠nea\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d3825b-0857-44e1-bf6a-d8c7a9032704",
   "metadata": {},
   "source": [
    "Let's return to our tweet data for an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53a81ea9-65c4-474a-8530-35393555d1be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\r\\nit's really the only bad thing about flying VA\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the second example\n",
    "second_example = tweets['text'][5] # Selecciona el texto del tweet que est√° en la posici√≥n 5 de la columna 'text'\n",
    "second_example # Muestra el contenido de ese tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef55865-36fd-4c06-a765-530cf3b53096",
   "metadata": {},
   "source": [
    "In this case, we don't really want to split the tweet into a list of strings. We still expect a single string of text but would like to remove the line break completely from the string.\n",
    "\n",
    "The string method `.strip()` effectively does the job of stripping away spaces at both ends of the text. However, it won't work in our example as the newline character is in the middle of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b933503b-4370-4dc4-b287-6dc2f9cdb1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing.\\r\\nit's really the only bad thing about flying VA\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Strip only removed blankspace at both ends\n",
    "second_example.strip() # Elimina los espacios en blanco al inicio y al final del tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99b80b4-804f-460f-a2d5-adbd654902b3",
   "metadata": {},
   "source": [
    "This is where regex could be really helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ceac9714-7053-4b2e-affb-71f8c3d2dcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # Buscar todas las palabras que empiezan con 't' en un texto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f08d20-ba81-4e48-9e2a-5728148005b3",
   "metadata": {},
   "source": [
    "Now, with regex, we are essentially calling it to match a pattern that we have identified in the text data, and we want to do some operations to the matched part‚Äîextract it, replace it with something else, or remove it completely. Therefore, the way regex works could be unpacked into the following steps:\n",
    "\n",
    "- Identify and write the pattern in regex (`r'PATTERN'`)\n",
    "- Write the replacement for the pattern (`'REPLACEMENT'`)\n",
    "- Call the specific regex function (e.g., `re.sub()`)\n",
    "\n",
    "In our example, the pattern we are looking for is `\\s`, which is the regex short name for any whitespace character (`\\n` and `\\t` included). We also add a quantifier `+` to the end: `\\s+`. It means we'd like to capture one or more occurences of the whitespace character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1248d227-1149-4014-94a5-c05592a27a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a pattern in regex\n",
    "blankspace_pattern = r'\\s+' # Definir un patr√≥n de expresi√≥n regular para uno o m√°s espacios en blanco\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc075c2e-1a1d-4393-a3ea-8ad7c118364b",
   "metadata": {},
   "source": [
    "The replacement for one or more whitespace characters is exactly one single whitespace, which is the canonical word boundary in English. Any additional whitespace will be reduced to a single whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55cb2f1-f4ca-4b79-900c-f65ec303ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a replacement for the pattern identfied\n",
    "blankspace_repl = ' ' # Definir el reemplazo para el patr√≥n de espacios en blanco: un solo espacio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12e3d1-728a-429b-9c83-4dcc88590bc4",
   "metadata": {},
   "source": [
    "Lastly, let's put everything together using the function [`re.sub()`](https://docs.python.org/3.11/library/re.html#re.sub), which means we want to substitute a pattern with a replacement. The function takes in three arguments‚Äîthe pattern, the replacement, and the string to which we want to apply the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5249b24b-7111-4569-be29-c40efa5e148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second_example\n"
     ]
    }
   ],
   "source": [
    "# Replace whitespace(s) with ' '\n",
    "clean_text = re.sub( # Reemplaza uno o m√°s espacios en blanco en 'second_example' por un solo espacio\n",
    "    pattern = blankspace_pattern, # Patr√≥n que busca uno o m√°s espacios en blanco\n",
    "    repl = blankspace_repl, # Reemplazo: un solo espacio\n",
    "    string = 'second_example') # Texto original donde se aplica el reemplazo\n",
    "print(clean_text) # Imprime el tweet limpio con los espacios normalizados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a895fbe3-a034-4124-94af-72a528913c51",
   "metadata": {},
   "source": [
    "Ta-da! The newline character is no longer there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7087dc0c-5fef-4f1c-8662-7cbc8a978f34",
   "metadata": {},
   "source": [
    "### Remove Punctuation Marks\n",
    "\n",
    "Sometimes we are only interested in analyzing **alphanumeric characters** (i.e., the letters and numbers), in which case we might want to remove punctuation marks. \n",
    "\n",
    "The `string` module contains a list of predefined punctuation marks. Let's print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70e8502b-b703-45e0-8852-0c3210363440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Load in a predefined list of punctuation marks\n",
    "from string import punctuation # Importa una lista predefinida de signos de puntuaci√≥n de la librer√≠a est√°ndar 'string'\n",
    "print(punctuation) # Muestra todos los caracteres de puntuaci√≥n disponibles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91119c9e-431c-42cb-afea-f7e607698929",
   "metadata": {},
   "source": [
    "In practice, to remove these punctuation characters, we can simply iterate over the text and remove characters found in the list, such as shown below in the `remove_punct` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237d868d-339d-4bbe-9a3b-20fa5fbdf231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    '''Remove punctuation marks in input text'''\n",
    "    \n",
    "    # Select characters not in puncutaion\n",
    "    no_punct = []     # Crear una lista para guardar los caracteres que NO son puntuaci√≥n\n",
    "    for char in text:     # Itera sobre cada car√°cter en el texto\n",
    "        if char not in punctuation: # Si el car√°cter no est√° en la lista de puntuaci√≥n\n",
    "            no_punct.append(char) # lo a√±ade a la lista\n",
    "\n",
    "\n",
    "\n",
    "    # Join the characters into a string\n",
    "    text_no_punct = ''.join(no_punct) # Une los caracteres filtrados de nuevo en un solo string  \n",
    "    \n",
    "    return text_no_punct # Devuelve el texto limpio sin signos de puntuaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc768b-c2dd-4386-8212-483c4485e4be",
   "metadata": {},
   "source": [
    "Let's apply the function to the example below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7596c465-3d85-4b72-a853-f2151bcd91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the third example\n",
    "third_example = tweets['text'][20] # Selecciona el texto del tweet que est√° en la posici√≥n 20 de la columna 'text'\n",
    "print(third_example) # Imprime ese tweet original\n",
    "print(f\"{'=' * 50}\") # Imprime una l√≠nea separadora de 50 signos \"=\"\n",
    "\n",
    "# Apply the function \n",
    "clean_third_example = remove_punct(third_example) # Aplica la funci√≥n remove_punct para eliminar la puntuaci√≥n del tweet\n",
    "remove_punct(third_example) # Imprime el tweet limpio sin signos de puntuaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853a4b83-f503-4405-aedd-66bbc088e3e7",
   "metadata": {},
   "source": [
    "Let's give it a try with another tweet. What have you noticed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3c2f60-fc92-4326-bad6-5ad04be50476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "# Print another tweet\n",
    "print(tweets['text'][100]) # Imprime el tweet que est√° en la posici√≥n 100 de la columna 'text'\n",
    "print(f\"{'=' * 50}\") # Imprime una l√≠nea separadora de 50 signos \"=\" para mejorar la visualizaci√≥n\n",
    "# Apply the function\n",
    "clean_example_100 = remove_punct(tweets['text'][100]) # Aplica la funci√≥n remove_punct para eliminar la puntuaci√≥n del tweet\n",
    "print(clean_example_100)# Muestra el tweet limpio sin signos de puntuaci√≥n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af02ce5-b674-4cb4-8e08-7d7416963f9c",
   "metadata": {},
   "source": [
    "What about the following example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8c3947-e6b8-42fe-8a58-15e4b6c60005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weve got quite a bit of punctuation here dont we Python DLab\n"
     ]
    }
   ],
   "source": [
    "# Print a text with contraction\n",
    "contraction_text = \"We've got quite a bit of punctuation here, don't we?!? #Python @D-Lab.\"# Texto de ejemplo que contiene contracciones, puntuaci√≥n, hashtags y menciones\n",
    "\n",
    "# Apply the function\n",
    "clean_contraction_text = remove_punct(contraction_text) # Aplica la funci√≥n remove_punct para eliminar todos los signos de puntuaci√≥n\n",
    "print(clean_contraction_text)# Muestra el texto limpio sin puntuaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62574c66-db3f-4500-9c3b-cea2f3eb2a30",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Warning:** In many cases, we want to remove punctuation marks **after** tokenization, which we will discuss in a minute. This tells us that the **order** of preprocessing is a matter of importance!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6b85e-58e7-4f56-9b4a-b60c85b394ba",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 1: Preprocessing with Multiple Steps\n",
    "\n",
    "So far we've learned a few preprocessing operations, let's put them together in a function! This function would be a handy one to refer to if you happen to work with some messy English text data, and you want to preprocess it with a single function. \n",
    "\n",
    "The example text data for challenge 1 is shown below. Write a function to:\n",
    "- Lowercase the text\n",
    "- Remove punctuation marks\n",
    "- Remove extra whitespace characters\n",
    "\n",
    "Feel free to recycle the codes we've used above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb10cba-239e-4856-b56d-7d5eb850c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge1_path = '../data/example1.txt'# Ruta del archivo de texto que queremos leer\n",
    "\n",
    "with open(challenge1_path, 'r') as file:# Abre el archivo en modo lectura ('r') y lo carga en la variable 'challenge1'\n",
    "    challenge1 = file.read() # Lee todo el contenido del archivo\n",
    "    \n",
    "print(challenge1)# Imprime el contenido del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2480823-65dd-4f52-a7b3-6d9b10d87912",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):   #Limpia un texto aplicando min√∫sculas, eliminando puntuaci√≥n y normalizando espacios\n",
    "\n",
    "    # Step 1: Lowercase \n",
    "    text = ...    # Step 1: Convertir todo el texto a min√∫sculas\n",
    "\n",
    "\n",
    "    # Step 2: Use remove_punct to remove punctuation marks\n",
    "    text = ...    # Step 2: Usar remove_punct para eliminar signos de puntuaci√≥n\n",
    "\n",
    "    # Step 3: Remove extra whitespace characters\n",
    "    text = ...    # Step 3: Eliminar espacios extra usando el patr√≥n y reemplazo definidos\n",
    "\n",
    "    return text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc603506-0adb-45d7-bb6f-62958c054fdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Uncomment to apply the above function to challenge 1 text \n",
    "clean_text(challenge1) #Ejecuta la funci√≥n clean_text tomando como entrada el texto almacenado en la variable challenge1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c159cb-8eaa-4c30-b8ff-38a712d2bb0f",
   "metadata": {},
   "source": [
    "## Task-specific Processes\n",
    "\n",
    "Now that we understand common preprocessing operations, there are still a few additional operations to consider. Our text data might require further normalization depending on the language, source, and content of the data.\n",
    "\n",
    "For example, if we are working with financial documents, we might want to standardize monetary symbols by converting them to digits. It our tweets data, there are numerous hashtags and URLs. These can be replaced with placeholders to simplify the subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2936cea-74e9-40c2-bfbe-6ba8129330de",
   "metadata": {},
   "source": [
    "### üé¨ **Demo**: Remove Hashtags and URLs \n",
    "\n",
    "Although URLs, hashtags, and numbers are informative in their own right, oftentimes we don't necessarily care about the exact meaning of each of them. \n",
    "\n",
    "While we could remove them completely, it's often informative to know that there **exists** a URL or a hashtag. In practice, we replace individual URLs and hashtags with a \"symbol\" that preserves the fact these structures exist in the text. It's standard to just use the strings \"URL\" and \"HASHTAG.\"\n",
    "\n",
    "Since these types of text often follow a regular structure, they're an apt case for using regular expressions. Let's apply these patterns to the tweets data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c0dc37-a013-4f0a-b72f-a1f64dc6c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the example tweet \n",
    "url_tweet = tweets['text'][13] # Selecciona el tweet en la posici√≥n 13 de la columna 'text'\n",
    "print(url_tweet)# Imprime el contenido del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef61bea-ea11-468d-8176-a2f63659d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL \n",
    "url_pattern = r'(http|ftp|https):\\/\\/([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])'# Definir un patr√≥n de expresi√≥n regular para detectar URLs en el texto\n",
    "url_repl = ' URL ' # Definir el reemplazo por el texto ' URL '\n",
    "re.sub(url_pattern, url_repl, url_tweet) # Reemplaza todas las URLs en 'url_tweet' por la cadena ' URL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e0f2a-460e-4088-aa89-dc2a8bc6f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hashtag\n",
    "hashtag_pattern = r'(?:^|\\s)[ÔºÉ#]{1}(\\w+)' # Definir un patr√≥n de expresi√≥n regular para detectar hashtags en el texto\n",
    "hashtag_repl = ' HASHTAG '# Definir el reemplazo por la cadena ' HASHTAG '\n",
    "re.sub(hashtag_pattern, hashtag_repl, url_tweet)# Mostrar el resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d68d49-4923-49c0-9113-b844dc7546b9",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "\n",
    "# Tokenization\n",
    "\n",
    "## Tokenizers Before LLMs\n",
    "\n",
    "One of the most important steps in text analysis is tokenization. This is the process of breaking a long sequence of text into word tokens. With these tokens available, we are ready to perform word-level analysis. For instance, we can filter out tokens that don't contribute to the core meaning of the text.\n",
    "\n",
    "In this section, we'll introduce how to perform tokenization using `nltk`, `spaCy`, and a Large Language Model (`bert`). The purpose is to expose you to different NLP packages, help you understand their functionalities, and demonstrate how to access key functions in each package.\n",
    "\n",
    "### `nltk`\n",
    "\n",
    "The first package we'll be using is called **Natural Language Toolkit**, or `nltk`. \n",
    "\n",
    "Let's install a couple modules from the package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d81f8-361e-4273-bd36-91a272f4a38a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk #Importa la librer√≠a NLTK (Natural Language Toolkit) en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b327cc-5c77-4fdc-9aaf-17d7f0761237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Uncomment the following lines to install these modules \n",
    "nltk.download('wordnet') #descarga el diccionario WordNet, usado para lematizaci√≥n\n",
    "nltk.download('stopwords') #descarga listas de stopwords\n",
    "nltk.download('punkt') #descarga el modelo de tokenizaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e79b699-c3a5-489f-9b3c-95653aba34d6",
   "metadata": {},
   "source": [
    "`nltk` has a function called `word_tokenize`. It requires one argument, which is the text to be tokenized, and it returns a list of tokens for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d6944-c641-4fac-a239-5947a496371c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica Really missed a prime opportunity for Men Without Hats parody, there. https://t.co/mWpG7grEZP\n"
     ]
    }
   ],
   "source": [
    "# Load word_tokenize \n",
    "from nltk.tokenize import word_tokenize # Importa la funci√≥n para dividir texto en tokens (palabras y signos)\n",
    "\n",
    "# Print the example\n",
    "text = tweets['text'][7] # Divide el tweet en tokens\n",
    "print(text) # Imprime la lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fde2a3-e4e2-4e61-ad54-e4d5d0a6ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the NLTK tokenizer\n",
    "nltk_tokens = word_tokenize(text) # Divide el texto del tweet en una lista de tokens (palabras, signos, etc.)\n",
    "nltk_tokens # Devuelve la lista de tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ead039-7721-4b22-8590-0d7824631675",
   "metadata": {},
   "source": [
    "Here we are, with a list of tokens identified by `nltk`. Let's take a minute to inspect them! \n",
    "\n",
    "üîî **Question**: Do word boundaries decided by `nltk` make sense to you? Pay attention to the twitter handle and the URL in the example tweet. \n",
    "\n",
    "You may feel that accessing functions in `nltk` is pretty straightforward. The function we used above was imported from the `nltk.tokenize` module, which as the name suggests, primarily does the job of tokenization. \n",
    "\n",
    "Underlyingly, `nltk` has [a collection of modules](https://www.nltk.org/api/nltk.html) that fulfill different purposes, to name a few:\n",
    "\n",
    "| NLTK module   | Fucntion                  | Link                                                         |\n",
    "|---------------|---------------------------|--------------------------------------------------------------|\n",
    "| nltk.tokenize | Tokenization              | [Documentation](https://www.nltk.org/api/nltk.tokenize.html) |\n",
    "| nltk.corpus   | Retrieve built-in corpora | [Documentation](https://www.nltk.org/nltk_data/)             |\n",
    "| nltk.tag      | Part-of-speech tagging    | [Documentation](https://www.nltk.org/api/nltk.tag.html)      |\n",
    "| nltk.stem     | Stemming                  | [Documentation](https://www.nltk.org/api/nltk.stem.html)     |\n",
    "| ...           | ...                       | ...                                                          |\n",
    "\n",
    "Let's import `stopwords` from the `nltk.corpus` module, which hosts a range of built-in corpora. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84bbfced-8803-41ca-9cae-49bdadf8c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predefined stop words from nltk\n",
    "from nltk.corpus import stopwords # importa desde NLTK la lista predefinida de stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee971a1-1189-4cb6-8317-4836f54c3ae2",
   "metadata": {},
   "source": [
    "Let's specificy that we want to retrieve English stop words. The function simply returns a list of stop words, mostly function words, that `nltk` identifies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6009e1df-b720-4d22-a162-7fd250a58672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first 10 stopwords\n",
    "stop = stopwords.words('english') #Cargar la lista de palabras vac√≠as (stopwords) en ingl√©s desde NLTK\n",
    "stop[:10] #Mostrar las primeras 10 palabras vac√≠as de la lista\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3ec908-de6c-42c5-a370-f1b1df0032b3",
   "metadata": {},
   "source": [
    "### `spaCy`\n",
    "Other than `nltk`, we have another widely-used package called `spaCy`. \n",
    "\n",
    "`spaCy` has its own processing pipeline. It takes in a string of text, runs the `nlp` pipeline on it, and stores the processed text and its annotations in an object called `doc`. The `nlp` pipeline always performs tokenization, as well as [other text analysis components](https://spacy.io/usage/processing-pipelines#custom-components) requested by the user. These components are pretty similar to modules in `nltk`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a0facd-4b75-41ac-920c-5ea044f7ae2e",
   "metadata": {},
   "source": [
    "<img src='../images/spacy.png' alt=\"spacy pipeline\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ef1eaf-2790-4928-b094-943f2803c6a0",
   "metadata": {},
   "source": [
    "Note that we always start by initializing the `nlp` pipeline, depending on the language of the text. Here, we are loading a pretrained language model for English: `en_core_web_sm`. The name suggests that it is a lightweight model trained on some text data (e.g., blogs); see model descriptions [here](https://spacy.io/models/en#en_core_web_sm).\n",
    "\n",
    "This is the first time we encounter the concept of **pretraining**, though you may have heard it elsewhere. In the context of NLP, pretraining means that the model has been trained on a vast amount of data. As a result, it comes with a certain \"knowledge\" of word structure and grammar of the language.\n",
    "\n",
    "Therefore, when we apply the model to our own data, we can expect it to be reasonably accurate in performing various annotation tasks, e.g., tagging a word's part of speech, identifying the syntactic head of a phrase, and etc. \n",
    "\n",
    "Let's dive in! We'll first need to load the pretrained language model we installed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "524dfc02-aa8f-4888-9f81-74a570db72b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # Importa la librer√≠a spaCy para procesamiento de lenguaje natural (NLP)\n",
    "nlp = spacy.load('en_core_web_sm') # Carga el modelo en ingl√©s \"small\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d669c3-2f5a-41b6-893b-ea1d438b3a48",
   "metadata": {},
   "source": [
    "The `nlp` pipeline, by default, includes a set of components, which we can access via the `.pipe_names` attribute. \n",
    "\n",
    "You may notice that it dosen't include the tokenizer. Don't worry! Tokenizer is a special component that the pipeline always includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d581ca5-43f8-4ef9-b099-2fc92c324581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve components included in NLP pipeline\n",
    "nlp.pipe_names # Recupera los nombres de los componentes que forman parte del pipeline de procesamiento de spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e37f91-d174-4101-bfc6-2859cb0fe5cc",
   "metadata": {},
   "source": [
    "Let's run the `nlp` pipeline on our example tweet data, and assign it to a variable `doc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618e8558-625d-4546-8109-63f9bae9790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the pipeline to example tweet\n",
    "doc = nlp(tweets['text'][7]) # Aplicar el pipeline de spaCy al tweet n√∫mero 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54325d60-5c5c-488d-baf2-7eed4de2c031",
   "metadata": {},
   "source": [
    "Under the hood, the `doc` object contains the tokens (created by the tokenizer) and their annotations (created by other components), which are [linguistic features](\n",
    "https://spacy.io/usage/linguistic-features) useful for text analysis. We retrieve the token and its annotations by accessing corresponding attributes. \n",
    "\n",
    "| Attribute      | Annotation                              | Link                                                                      |\n",
    "|----------------|-----------------------------------------|---------------------------------------------------------------------------|\n",
    "| token.text     | The token in verbatim text              | [Documentation](https://spacy.io/api/token#attributes)                    |\n",
    "| token.is_stop  | Whether the token is a stop word        | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.is_punct | Whether the token is a punctuation mark | [Documentation](https://spacy.io/api/attributes#_title)                   |\n",
    "| token.lemma_   | The base form of the token              | [Documentation](https://spacy.io/usage/linguistic-features#lemmatization) |\n",
    "| token.pos_     | The simple POS-tag of the token         | [Documentation](https://spacy.io/usage/linguistic-features#pos-tagging)   |\n",
    "| ...            | ...                                     | ...                                                                       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9f23c8-a157-44a7-a6ec-6894aec1a595",
   "metadata": {},
   "source": [
    "Let's first get the tokens themselves! We'll iterate over the `doc` object and retrieve the text of each token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c71efee-e6cf-46c4-9198-593304f6560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the verbatim texts of tokens\n",
    "spacy_tokens = [token.text for token in doc] # Obtener el texto literal de cada token en el objeto Doc de spaCy\n",
    "\n",
    "spacy_tokens # Mostrar la lista de tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fc23f0-c699-45e6-ad62-e131036d601f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'VirginAmerica',\n",
       " 'Really',\n",
       " 'missed',\n",
       " 'a',\n",
       " 'prime',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'Men',\n",
       " 'Without',\n",
       " 'Hats',\n",
       " 'parody',\n",
       " ',',\n",
       " 'there',\n",
       " '.',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/mWpG7grEZP']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the NLTK tokens\n",
    "nltk_tokens  #Una lista de tokens obtenida usando la funci√≥n de NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ace59e-40e0-42b3-9f2b-d30ac94dccab",
   "metadata": {},
   "source": [
    "üîî **Question**: Let's pause for a minute to compare the tokens generated by `nltk` and `spaCy`. What have you noticed?\n",
    "\n",
    "Remember we can also access various annotations of these okens. For instance, one annotation `spaCy` offers is that it conveniently encodes whether a token is a stop word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626af687-e986-4c97-af86-edf7dbd22c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the is_stop annotation\n",
    "spacy_stops = [token.is_stop for token in doc] # Recupera la anotaci√≥n 'is_stop' de cada token en el objeto Doc de spaCy\n",
    "\n",
    "# The results are boolean values\n",
    "spacy_stops # Muestra la lista de valores booleanos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6548b6-7e89-4f42-b8cb-bf7c93b34eb4",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 2: Remove Stop Words\n",
    "\n",
    "We have known how `nltk` and `spaCy` work as NLP packages. We've also demostrated how to identify stop words with each package. \n",
    "\n",
    "Let's write **two** functions to remove stop words from our text data. \n",
    "\n",
    "- Complete the function for stop words removal using `nltk`\n",
    "    - The starter code requires two arguments: the raw text input and a list of predefined stop words\n",
    "- Complete the function for stop words removal using `spaCy`\n",
    "    - The starter code requires one argument: the raw text input\n",
    " \n",
    "A friendly reminder before we dive in: both functions take raw text as input‚Äîthat's a signal to perform tokenization on the raw text first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b24b5370-392f-420d-8c9e-78146d0fca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_nltk(raw_text, stopword):\n",
    "    \"\"\"\n",
    "    Elimina stopwords de un texto usando NLTK.\n",
    "    \"\"\"\n",
    "    # Step 1: Tokenization with nltk\n",
    "    # YOUR CODE HERE \n",
    "    tokens = word_tokenize(raw_text) # Divide el texto en palabras y signos de puntuaci√≥n\n",
    "    \n",
    "    # Step 2: Filter out tokens in the stop word list\n",
    "    # YOUR CODE HERE\n",
    "    clean_tokens = [token for token in tokens if token.lower() not in stopword] # Crea una nueva lista de tokens excluyendo aquellos que est√©n en la lista de stopwords\n",
    "\n",
    "    return clean_tokens   # Devolver la lista de tokens limpios\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65b5bbda-0af5-49a9-8f5e-77d61ab217e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopword_spacy(raw_text):\n",
    "\n",
    "    # Step 1: Apply the nlp pipeline\n",
    "    # YOUR CODE HERE\n",
    "    doc = nlp(raw_text) # Convierte el texto en un objeto Doc que contiene tokens y atributos ling√º√≠sticos\n",
    "    \n",
    "    # Step 2: Filter out tokens that are stop words\n",
    "    # YOUR CODE HERE\n",
    "    clean_tokens = [token.text for token in doc if not token.is_stop and not token.is_space] # Crea una lista de tokens del objeto Doc de spaCy, excluyendo las stopwords y los espacios\n",
    "\n",
    "    return clean_tokens   # Devolver la lista de tokens limpios\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c3b0da-2223-4d7f-9014-696498e804e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_nltk(text, stop) # Llama a la funci√≥n remove_stopword_nltk para eliminar las stopwords de 'text' usando la lista 'stop'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83538ba-6bf1-49ca-90ec-b6532b1ffcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stopword_spacy(text) # Llama a la funci√≥n remove_stopword_spacy para eliminar las stopwords de 'text' usando spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a6b1ec-87cc-4a08-a5dd-0210a9c56f0b",
   "metadata": {},
   "source": [
    "## üé¨ **Demo**: Powerful Features from `spaCy`\n",
    "\n",
    "`spaCy`'s nlp pipeline includes a number of linguistic annotations, which could be very useful for text analysis. \n",
    "\n",
    "For instance, we can access more annotations such as the lemma, the part-of-speech tag and its meaning, and whether the token looks like URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c7d93-51a3-4fb8-8321-cb672f4f1b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print tokens and their annotations\n",
    "for token in doc: # Imprimir tokens y sus anotaciones ling√º√≠sticas del objeto Doc de spaCy\n",
    "    print(f\"{token.text:<24} | {token.lemma_:<24} | {token.pos_:<12} | {spacy.explain(token.pos_):<12} | {token.like_url:<12} |\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17388e0c-88b6-4cd9-8d2b-adb7f10b5330",
   "metadata": {},
   "source": [
    "As you can imagine, it is typical for this dataset to contain place names and airport codes. It would be cool if we are able to identify them and extract them from tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb489f78-fbb2-497b-a36d-3400c00c9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print example tweets with place names and airport codes\n",
    "tweet_city = tweets['text'][8273]  # Tweet con nombres de ciudades\n",
    "tweet_airport = tweets['text'][502] # Tweet con c√≥digos de aeropuertos\n",
    "print(tweet_city)  # Imprimir el tweet con nombres de ciudades\n",
    "print(f\"{'=' * 50}\")  # L√≠nea separadora para mayor claridad\n",
    "print(tweet_airport) # Imprimir el tweet con c√≥digos de aeropuertos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013990a5-5e07-4a45-9427-fcd33840d3b8",
   "metadata": {},
   "source": [
    "We can use the \"ner\" ([Named Entity Recognition](https://en.wikipedia.org/wiki/Named-entity_recognition)) component to identify entities and their categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9e63519-5991-49fa-9a5d-be9f9b408ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva York      | 14         | 24         | GPE       \n",
      "Par√≠s el pr√≥ximo verano | 27         | 50         | GPE       \n"
     ]
    }
   ],
   "source": [
    "tweet_city = \"Voy a visitar Nueva York y Par√≠s el pr√≥ximo verano.\"\n",
    "# Print entities identified from the text\n",
    "doc_city = nlp(tweet_city) # Procesar el tweet que contiene nombres de ciudades con spaCy\n",
    "# Recorrer las entidades nombradas detectadas y mostrarlas en formato tabular\n",
    "for ent in doc_city.ents:\n",
    "    print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b933ed0-7018-450c-b0a6-fb76cb6d5be9",
   "metadata": {},
   "source": [
    "We can also use `displacy` to highlight entities identified in the text, and at the same time, annotate the entity category. \n",
    "\n",
    "In the following example, we have four `GPE` (i.e., geopolitical entities, usually countries and cities) identified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5a5219-af1f-445c-a35b-7c49d739a91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "# Visualize the identified entities\n",
    "from spacy import displacy # Importar displacy para visualizaci√≥n de entidades\n",
    "displacy.render(doc_city, style='ent', jupyter=True) # Visualizar las entidades nombradas del tweet procesado en Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7953b-04a1-46d0-817a-db29edd8c83b",
   "metadata": {},
   "source": [
    "Let's give it a try with another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9246c7e3-8990-4d47-a355-deb63dbd1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print entities identified from the text\n",
    "doc_airport = nlp(tweet_airport) # Analiza el texto del tweet con el modelo de spaCy\n",
    "# Recorre todas las entidades (entidades nombradas) que spaCy identific√≥ en el texto\n",
    "for ent in doc_airport.ents:\n",
    "     print(f\"{ent.text:<15} | {ent.start_char:<10} | {ent.end_char:<10} | {ent.label_:<10}\") #Imprime en columnas: el texto de la entidad, su posici√≥n inicial, final y la etiqueta de la entidad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df38472-3193-44c5-9b2f-e311ce9d42e0",
   "metadata": {},
   "source": [
    "Interesting that airport codes are identified as `ORG`‚Äîorganizations, and the tweet handle as `CARDINAL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4bf382-4c57-4b78-a37f-fc1f6cb1d565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    @VirginAmerica\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Flying LAX\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SFO\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and after looking at the awesome movie lineup I actually wish I was on a long haul.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the identified entities\n",
    "displacy.render(doc_airport, style='ent', jupyter=True)# Muestra en Jupyter las entidades nombradas (NER) detectadas en el texto con resaltado visual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467d6f28-effc-4fe1-90e9-47c89dc5492d",
   "metadata": {},
   "source": [
    "## Tokenizers Since LLMs\n",
    "\n",
    "So far, we've seen what tokenization looks like with two widely-used NLP packages. They work quite well in some settings, but not others. Recall that `nltk` struggles with URLs. Now, imagine the data we have is even messier, containing misspellings, recently coined words, foreign names, and etc (collectively called \"out of vocabulary\" or OOV words). In such circumstances, we might need a more powerful model to handle these complexities.\n",
    "\n",
    "In fact, tokenization schemes change substantially with **Large Language Models** (LLMs), which are models trained on an enormous amount of data from mixed sources. With that magnitude of data, LLMs are better at chunking a longer sequence into tokens and tokens into **subtokens**. These subtokens can be morphological units of a word, such as an affix, but they can also be parts of a word where the model sets a \"meaningful\" boundary. \n",
    "\n",
    "In this section, we will demonstrate tokenization in **BERT** (Bidirectional Encoder Representations from Transformers), which utilizes a tokenization algorithm called [**WordPiece**](https://huggingface.co/learn/nlp-course/en/chapter6/6). \n",
    "\n",
    "We will load the tokenizer of BERT from the package `transformers`, which hosts a number of Transformer-based LLMs (e.g., BERT). We won't go into the architecture of Transformer in this workshop, but feel free to check out the D-lab workshop on [GPT Fundamentals](https://github.com/dlab-berkeley/GPT-Fundamentals)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1509b-5b9e-456d-909f-a6b5099c48c8",
   "metadata": {},
   "source": [
    "### WordPiece Tokenization\n",
    "\n",
    "Note that BERT comes in a variety of versions. The one we will explore today is `bert-base-uncased`. This model has a moderate size (referred to as `base`) and is case-insensitive, meaning the input text will be lowercased by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7927226c-d04e-4117-9c49-3d355611b209",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load BERT tokenizer in\n",
    "from transformers import BertTokenizer # Importa el tokenizador de BERT desde Hugging Face\n",
    "\n",
    "# Initialize the tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Inicializa el tokenizador preentrenado 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128cfb38-274e-4d75-9d14-8744020fe49c",
   "metadata": {},
   "source": [
    "The tokenizer has multiple functions, as we will see in a minute. Now we want to access the `.tokenize()` function from the tokenizer. \n",
    "\n",
    "Let's tokenize an example tweet below. What have you noticed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62649193-bb00-4ae8-9102-bce3d1dfb6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an example tweet from dataframe\n",
    "text = tweets['text'][194] # Selecciona el texto del tweet n√∫mero 194 en el DataFrame \"tweets\"\n",
    "print(f\"Text: {text}\")# Imprime el texto del tweet\n",
    "print(f\"{'=' * 50}\") # Imprime una l√≠nea de separaci√≥n con 50 signos \"=\"\n",
    "\n",
    "\n",
    "# Apply tokenizer\n",
    "tokens = tokenizer.tokenize(text)# Aplica el tokenizador de BERT al texto para dividirlo en sub-palabras/tokens\n",
    "print(f\"Tokens: {tokens}\")# Imprime la lista de tokens resultantes\n",
    "print(f\"Number of tokens: {len(tokens)}\")# Imprime la cantidad total de tokens generados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4f780e-207c-4d3d-b1b6-063c6d118945",
   "metadata": {},
   "source": [
    "The double \"hashtag\" symbols (`##`) refer to a subword token‚Äîa segment separated from the previous token.\n",
    "\n",
    "üîî **Question**: Do these subwords make sense to you? \n",
    "\n",
    "One significant development with LLMs is that each token is assigned an ID from its vocabulary. Our computer does not understand text in its raw form, so each token is translated into an ID. These IDs are the inputs that the model accesses and operates on.\n",
    "\n",
    "Tokens and IDs can be converted bidirectionally, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3112a63d-82b6-4fc0-a904-ab86f8740653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID of just is: 2074\n",
      "Token 2074 is: just\n"
     ]
    }
   ],
   "source": [
    "# Get the input ID of the word \n",
    "print(f\"ID of just is: {tokenizer.vocab['just']}\") # Obtiene el ID en el vocabulario de BERT correspondiente a la palabra 'just'\n",
    "\n",
    "# Get the text of the input ID\n",
    "print(f\"Token 2074 is: {tokenizer.decode([2074])}\")# Decodifica el ID 2074 del vocabulario de BERT y muestra la palabra correspondiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9fd13d-f26e-480c-b43e-2b5fbc4898cd",
   "metadata": {},
   "source": [
    "Let's convert tokens to input IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d125e1d-2560-4136-829c-b1c11e34636c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input IDs: 4\n",
      "Input IDs of text: [101, 100, 100, 1016]\n"
     ]
    }
   ],
   "source": [
    "# Convert a list of tokens to a list of input IDs\n",
    "tokens = ['[CLS]', 'Hola', 'Daniela', '2']\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)# Convierte la lista de tokens generados por BERT a sus correspondientes IDs en el vocabulario\n",
    "print(f\"Number of input IDs: {len(input_ids)}\")# Imprime la cantidad total de IDs generados\n",
    "print(f\"Input IDs of text: {input_ids}\")# Imprime la lista completa de IDs que representan el texto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25a1a14-a6db-414b-a1a8-c6be73c81a15",
   "metadata": {},
   "source": [
    "### Special Tokens\n",
    "\n",
    "In addition to the tokens and subtokens discussed above, BERT also makes use of three special tokens: `SEP`, `CLS`, and `UNK`. The `SEP` token acts as a sentence terminator, commonly known as an `EOS` (End of Sentence) token. The `UNK` token represents any token that is not found in the vocabulary, hence \"unknown\" tokens. The `CLS` token is added to the beginning of the sentence. It originates from text classification tasks (e.g., spam detection), where reseachers found it useful to have a token that aggregates the information of the entire sentence for classification purposes.\n",
    "\n",
    "When we apply `tokenizer()` directly to our text data, we are asking BERT to **encode** the text for us. This involves multiple steps: \n",
    "- Tokenize the text\n",
    "- Add special tokens\n",
    "- Convert tokens to input IDs\n",
    "- Other model-specific processes\n",
    "  \n",
    "Let's print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21479c25-7a9a-4fac-ba09-1812575b8170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input IDs: 5\n",
      "IDs from tokenizer: [101, 7570, 2721, 25989, 102]\n"
     ]
    }
   ],
   "source": [
    "# Get the input IDs by providing the key \n",
    "text = \"Hola mundo\"\n",
    "input_ids_from_tokenizer = tokenizer(text)['input_ids'] # Tokeniza el texto y obtiene directamente los input IDs usando el tokenizador de BERT\n",
    "print(f\"Number of input IDs: {len(input_ids_from_tokenizer)}\")# Imprime la cantidad total de IDs generados\n",
    "print(f\"IDs from tokenizer: {input_ids_from_tokenizer}\") # Imprime la lista completa de input IDs generados por el tokenizador"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6ea98e-7374-488a-a804-46cab166125c",
   "metadata": {},
   "source": [
    "It looks like we have two more tokens added: 101 and 102. \n",
    "\n",
    "Let's convert them to texts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82a18dc1-ca0d-4d5b-8ac6-f56cb44bf8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 101st token: [CLS]\n",
      "The 102nd token: [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Convert input IDs to texts\n",
    "print(f\"The 101st token: {tokenizer.convert_ids_to_tokens(101)}\")# Convierte el ID 101 a su token correspondiente en el vocabulario de BERT\n",
    "print(f\"The 102nd token: {tokenizer.convert_ids_to_tokens(102)}\")# Convierte el ID 102 a su token correspondiente en el vocabulario de BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d4e10-e96c-432a-a6c9-2c992d00fcde",
   "metadata": {},
   "source": [
    "As you can see, our text example is now a list of vocabulary IDs. In addtion to that, BERT adds the sentence terminator `SEP` and the beginning `CLS` token to the original text. BERT's tokenizer encodes tons of texts likewise; and afterwards, they are ready for further processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56be32-2a5f-441e-8283-de3e60705c0b",
   "metadata": {},
   "source": [
    "## ü•ä Challenge 3: Find the Word Boundary\n",
    "\n",
    "Now that we know tokenization in BERT often returns subwords. Let's try a few more examples. \n",
    "\n",
    "- What do you think is the correct boundary for splitting the following words into subwords?\n",
    "- What other examples have you tested?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1140b19-398c-44dd-829c-c922b0e6f7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(string):\n",
    "    '''Tokenzie the input string with BERT'''\n",
    "    tokens = tokenizer.tokenize(string) # Aplica el tokenizador de BERT al texto de entrada\n",
    "\n",
    "    return print(tokens)  # Imprime la lista de tokens generada\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c07c71e-5be2-4d91-9c6f-26de4145307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dl', '##ab']\n",
      "['co', '##vid']\n",
      "['hug', '##ga', '##ble']\n",
      "['37', '##8']\n",
      "['airplane']\n"
     ]
    }
   ],
   "source": [
    "# Abbreviations\n",
    "get_tokens('dlab')# Abreviaturas: tokeniza la palabra 'dlab'\n",
    "\n",
    "# OOV\n",
    "get_tokens('covid')# Palabra fuera del vocabulario (OOV): tokeniza 'covid'\n",
    "\n",
    "# Prefix\n",
    "get_tokens('huggable')# Prefijo: tokeniza 'huggable'\n",
    "\n",
    "# Digits\n",
    "get_tokens('378')# D√≠gitos: tokeniza '378'\n",
    "\n",
    "# YOUR EXAMPLE\n",
    "get_tokens('airplane')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb7cb9-6e60-4e0a-8cc6-21d57237e835",
   "metadata": {},
   "source": [
    "We will wrap up Part 1 with this (hopefully) thought-provoking challenge. LLMs often come with a much more sophisticated tokenization scheme, but there is ongoing discussion about their limitations in real-world applications. The reference section includes a few blog posts discussing this problem. Feel free to explore further if this sounds like an interesting question to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7943ed9-70de-4f4a-b1bb-b2896d05e618",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. A tutorial introducing the tokenization scheme in BERT: [The huggingface NLP course on wordpiece tokenization](https://huggingface.co/learn/nlp-course/chapter6/6?fw=pt)\n",
    "2. A specific example of \"failure\" in tokenization: [Weaknesses of wordpiece tokenization: Findings from the front lines of NLP at VMware.](https://medium.com/@rickbattle/weaknesses-of-wordpiece-tokenization-eb20e37fec99)\n",
    "3. How does BERT decide boundaries between subtokens: [Subword tokenization in BERT](https://tinkerd.net/blog/machine-learning/bert-tokenization/#subword-tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0812a7-f033-46ed-bc7b-67109c369e6c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "## ‚ùó Key Points\n",
    "\n",
    "* Preprocessing includes multiple steps, some of them are more common to text data regardlessly, and some are task-specific. \n",
    "* Both `nltk` and `spaCy` could be used for tokenization and stop word removal. The latter is more powerful in providing various linguistic annotations. \n",
    "* Tokenization works differently in BERT, which often involves breaking down a whole word into subwords. \n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_da",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
